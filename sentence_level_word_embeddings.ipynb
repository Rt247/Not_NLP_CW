{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentence-level-word-embeddings",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rt247/Not_NLP_CW/blob/BERT_method/sentence_level_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6BshlcaZgmT",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Download datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzekR1EZY_5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "42846080-a7e5-4a34-cf37-350887748745"
      },
      "source": [
        "from os.path import exists\n",
        "\n",
        "if not exists('enzh_data.zip'):\n",
        "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
        "    !unzip enzh_data.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-19 14:27:03--  https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=0c04ce13836af89df7bdde0ae9a04943355b13151112fada78ca89b9199ab942&X-Amz-Date=20200219T142704Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200219%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2020-02-19 14:27:04--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=0c04ce13836af89df7bdde0ae9a04943355b13151112fada78ca89b9199ab942&X-Amz-Date=20200219T142704Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200219%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 870893 (850K) [application/zip]\n",
            "Saving to: ‘enzh_data.zip’\n",
            "\n",
            "enzh_data.zip       100%[===================>] 850.48K   493KB/s    in 1.7s    \n",
            "\n",
            "2020-02-19 14:27:08 (493 KB/s) - ‘enzh_data.zip’ saved [870893/870893]\n",
            "\n",
            "Archive:  enzh_data.zip\n",
            "  inflating: dev.enzh.mt             \n",
            "  inflating: dev.enzh.scores         \n",
            "  inflating: dev.enzh.src            \n",
            "  inflating: test.enzh.mt            \n",
            "  inflating: test.enzh.src           \n",
            "  inflating: train.enzh.mt           \n",
            "  inflating: train.enzh.src          \n",
            "  inflating: train.enzh.scores       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UskRgN-6ZoKo",
        "colab_type": "text"
      },
      "source": [
        "Check data downloaded successfully:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQUZyQrXY_5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"./train.enzh.src\", \"r\") as enzh_src:\n",
        "  print(\"Source: \",enzh_src.readline())\n",
        "with open(\"./train.enzh.mt\", \"r\") as enzh_mt:\n",
        "  print(\"Translation: \",enzh_mt.readline())\n",
        "with open(\"./train.enzh.scores\", \"r\") as enzh_scores:\n",
        "  print(\"Score: \",enzh_scores.readline())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV-JSE2sZzoT",
        "colab_type": "text"
      },
      "source": [
        "### English Models Setup\n",
        "\n",
        "Download English models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkP0aR4rZbVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!spacy download en_core_web_md\n",
        "!spacy link en_core_web_md en300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYByRCqQaQDH",
        "colab_type": "text"
      },
      "source": [
        "Load a GloVe English model with dim 100.\n",
        "\n",
        "Some Chinese models only have **dim 100**, so we will need to **tokenize with spaCy, then embed with GloVe**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVfblxyVaO3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "import spacy\n",
        "\n",
        "# Embedding for English when dim 100\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
        "\n",
        "# Tokenizer for English when dim 100, Tokenizer and Embedding when dim 300\n",
        "nlp_en = spacy.load('en300')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tWbs9PucbS0",
        "colab_type": "text"
      },
      "source": [
        "Functions for processing English dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoKMJIr5acPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#downloading stopwords from the nltk package\n",
        "download('stopwords') #stopwords dictionary, run once\n",
        "stop_words_en = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def preprocess_en(sentence, nlp):\n",
        "    text = sentence.lower()\n",
        "    doc = [token.lemma_ for token in  nlp.tokenizer(text)]\n",
        "    doc = [word for word in doc if word not in stop_words_en]\n",
        "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
        "    return doc\n",
        "\n",
        "def get_word_vector_en(embeddings, word):\n",
        "    try:\n",
        "      vec = embeddings.vectors[embeddings.stoi[word]]\n",
        "      return vec\n",
        "    except KeyError:\n",
        "      #print(f\"Word {word} does not exist\")\n",
        "      pass\n",
        "      \n",
        "\n",
        "def get_sentence_emb_en(line, nlp):\n",
        "  text = line.lower()\n",
        "  l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
        "  l = ' '.join([word for word in l if word not in stop_words_en])\n",
        "\n",
        "  sen = nlp(l)\n",
        "  return sen.vector\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1930-s6fN7T",
        "colab_type": "text"
      },
      "source": [
        "### Chinese Models Setup\n",
        "\n",
        "Download Chinese stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O14b4JeHcNhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -c https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw437fJCip68",
        "colab_type": "text"
      },
      "source": [
        "Download and load Chinese model with **dim 100** (University of Oslo):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8V-nUwUikda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not exists(\"zh_100.zip\"):\n",
        "  !wget -O zh_100.zip http://vectors.nlpl.eu/repository/20/35.zip\n",
        "  !unzip zh_100.zip -d ./zh_100\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "wv_from_bin_100 = KeyedVectors.load_word2vec_format(\"./zh_100/model.bin\", binary=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo5vhydafzC5",
        "colab_type": "text"
      },
      "source": [
        "Functions for processing Chinese dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s27T7LXlf4sG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import jieba\n",
        "import gensim \n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "stop_words = [ line.rstrip() for line in open('./chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
        "\n",
        "def processing_zh(sentence):\n",
        "  seg_list = jieba.lcut(sentence,cut_all=True)\n",
        "  doc = [word for word in seg_list if word not in stop_words]\n",
        "  docs = [e for e in doc if e.isalnum()]\n",
        "  return docs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1zT8sEaqJaQ",
        "colab_type": "text"
      },
      "source": [
        "### BERT embedding Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xA7VWIl2pDU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "outputId": "b9a00742-9b95-4bba-ba43-9cd829ee04ef"
      },
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertModel, AdamW, BertConfig"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\r\u001b[K     |▊                               | 10kB 25.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 29.2MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 34.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40kB 38.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51kB 20.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 61kB 17.5MB/s eta 0:00:01\r\u001b[K     |████▉                           | 71kB 15.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 81kB 14.5MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 92kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 102kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 112kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 122kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 133kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 143kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 153kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 163kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 174kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 184kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 194kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 204kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 215kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 225kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 235kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 245kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 256kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 266kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 276kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 286kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 296kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 307kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 317kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 327kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 337kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 348kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 358kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 368kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 378kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 389kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 399kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 409kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 419kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 430kB 14.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 440kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 450kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 460kB 14.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 471kB 14.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 481kB 14.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.11.15)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 46.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 51.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 40.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.14.15)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=2557caff69772b652d1803c459e143b6619c1a8dd9eb926cfcee49739a0692fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3GCojqO4Qjp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c6e1cb1c-4f1e-4f9b-e1d9-112cf53efc97"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_oFKMjNqRa0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "4522eade-1aeb-4674-e811-c7339ea07d60"
      },
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "original_texts = open(\"./train.enzh.src\").readlines()\n",
        "translated_texts = open(\"./train.enzh.mt\").readlines()\n",
        "\n",
        "text_pairs = list(zip(original_texts, translated_texts))\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "inputs = [tokenizer.encode_plus(original, text_pair=translated, add_special_tokens = True, max_length=MAX_LENGTH, pad_to_max_length=True) for original, translated in text_pairs]\n",
        "input_ids = [d['input_ids'] for d in inputs]\n",
        "\n",
        "input_attention_masks = [d['attention_mask'] for d in inputs]\n",
        "\n",
        "#Scores\n",
        "f_train_scores = open(\"./train.enzh.scores\", 'r')\n",
        "zh_train_scores = f_train_scores.readlines()\n",
        "labels = np.array(zh_train_scores).astype(float)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n",
            "{'input_ids': [101, 10117, 12469, 25735, 11849, 11059, 48543, 10107, 10135, 10169, 10226, 79400, 34788, 119, 102, 4458, 2775, 5718, 3763, 4463, 6457, 8575, 5778, 2196, 5718, 2570, 6352, 6356, 2568, 7701, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
            "('The last conquistador then rides on with his sword drawn.\\n', '最后的征服者骑着他的剑继续前进.\\n')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Px5kVjo9nnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERILcV4iqYlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_BERT_embedding(batch_start, batch_end, input_tokens, attention_masks):\n",
        "  input_tensors = torch.tensor(input_tokens[batch_start:batch_end]).to(device)\n",
        "  attention_mask_tensors = torch.tensor(attention_masks[batch_start:batch_end]).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(input_tensors, attention_mask=attention_mask_tensors)\n",
        "  return last_hidden_states[0][:,0,:].cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WKpnrw3QtsB_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "c449cf6d-f898-463d-c6eb-b59cb927e02c"
      },
      "source": [
        "features = get_BERT_embedding(1000, 2000, input_ids, input_attention_masks)\n",
        "print(features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.25943062 -0.00346926  0.12020143 ...  0.04750847  0.07468242\n",
            "  -0.07719439]\n",
            " [-0.09621606  0.10552414 -0.14640783 ...  0.2795364   0.17046465\n",
            "   0.15454468]\n",
            " [-0.0317821   0.05737093  0.11677047 ...  0.1405592  -0.03532625\n",
            "  -0.04134069]\n",
            " ...\n",
            " [ 0.02738766 -0.11463747 -0.12570816 ...  0.08636823 -0.10662194\n",
            "  -0.0699797 ]\n",
            " [-0.19610456 -0.03125958  0.11257638 ...  0.03555025  0.03250408\n",
            "  -0.23782566]\n",
            " [-0.04782587 -0.18706174 -0.05254837 ...  0.29173648  0.09936409\n",
            "  -0.08668579]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI0BAckemQOU",
        "colab_type": "text"
      },
      "source": [
        "## Process Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ6nLoxEmWN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "\n",
        "f_train_scores = open(\"./train.enzh.scores\", 'r')\n",
        "zh_train_scores = f_train_scores.readlines()\n",
        "\n",
        "f_val_scores = open(\"./dev.enzh.scores\", 'r')\n",
        "zh_val_scores = f_val_scores.readlines()\n",
        "\n",
        "train_scores = np.array(zh_train_scores).astype(float)\n",
        "y_train_zh = train_scores\n",
        "\n",
        "val_scores = np.array(zh_val_scores).astype(float)\n",
        "y_val_zh = val_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLV1j47Vo97i",
        "colab_type": "text"
      },
      "source": [
        "## Support Vector Machines\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE4p089dsp1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup\n",
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from scipy.stats.stats import pearsonr\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF-AQtKJ4_p_",
        "colab_type": "text"
      },
      "source": [
        "### Using Average Word Embedding Vectors\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpgfIUEhoOAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_avg_sentence_vector_zh(line, word_vectors):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    try:\n",
        "      emb = word_vectors[w]\n",
        "      vectors.append(emb)\n",
        "    except:\n",
        "      pass #Do not add if the word is out of vocabulary\n",
        "  if vectors:\n",
        "    return np.mean(vectors, axis=0).tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "def get_avg_embeddings_zh(f, word_vectors):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "  for l in lines:\n",
        "    sent  = processing_zh(l)\n",
        "    vec = get_avg_sentence_vector_zh(sent, word_vectors)\n",
        "\n",
        "    if vec is not None:\n",
        "      sentences_vectors.append(vec)\n",
        "    else:\n",
        "      print(l)\n",
        "  return sentences_vectors\n",
        "\n",
        "\n",
        "def get_avg_sentence_vector_en(embeddings, line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    emb = get_word_vector_en(embeddings, w)\n",
        "    #do not add if the word is out of vocabulary\n",
        "    if emb is not None:\n",
        "      vectors.append(emb)\n",
        "  if vectors:\n",
        "    return torch.mean(torch.stack(vectors), dim=0).tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "# assume dim 100\n",
        "def get_avg_embeddings_en(f, embeddings, nlp):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "\n",
        "  for l in lines:\n",
        "    sentence = preprocess_en(l, nlp)\n",
        "    try:\n",
        "      vec = get_avg_sentence_vector_en(embeddings, sentence)\n",
        "      sentences_vectors.append(vec)\n",
        "    except:\n",
        "      sentences_vectors.append(0)\n",
        "\n",
        "  return sentences_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO1pLTEQpNya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zh_train_mt_100_a = get_avg_embeddings_zh(\"./train.enzh.mt\", wv_from_bin_100)\n",
        "zh_train_src_100_a = get_avg_embeddings_en(\"./train.enzh.src\", glove, nlp_en)\n",
        "\n",
        "zh_val_mt_100_a = get_avg_embeddings_zh(\"./dev.enzh.mt\", wv_from_bin_100)\n",
        "zh_val_src_100_a = get_avg_embeddings_en(\"./dev.enzh.src\", glove, nlp_en)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJH3H9XqphpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_100_a = [x + y for x, y in zip(zh_train_src_100_a, zh_train_mt_100_a)]\n",
        "X_train_zh_100_a = np.array(X_train_100_a)\n",
        "\n",
        "X_val_100_a = [x + y for x, y in zip(zh_val_src_100_a, zh_val_mt_100_a)]\n",
        "X_val_zh_100_a = np.array(X_val_100_a)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY1auosCuegb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_a, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_a)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "\"\"\"\n",
        "linear\n",
        "RMSE: 0.9044962563186333 Pearson 0.3017781690203462\n",
        "\n",
        "poly\n",
        "RMSE: 0.8990697909416231 Pearson 0.3032902746054339\n",
        "\n",
        "rbf\n",
        "RMSE: 0.8900985622788053 Pearson 0.3403404558003603\n",
        "\n",
        "sigmoid\n",
        "RMSE: 7.152607007355879 Pearson -0.03977439348067312\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nHytkDwlz3z",
        "colab_type": "text"
      },
      "source": [
        "### Using Sum of Word Embedding Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_3IKDI-l2iC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sum_sentence_vector_zh(line, word_vectors):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    try:\n",
        "      emb = word_vectors[w]\n",
        "      vectors.append(emb)\n",
        "    except:\n",
        "      pass #Do not add if the word is out of vocabulary\n",
        "  if vectors:\n",
        "    return np.sum(vectors, axis=0).tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "def get_sum_embeddings_zh(f, word_vectors):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "  for l in lines:\n",
        "    sent  = processing_zh(l)\n",
        "    vec = get_sum_sentence_vector_zh(sent, word_vectors)\n",
        "\n",
        "    if vec is not None:\n",
        "      sentences_vectors.append(vec)\n",
        "    else:\n",
        "      print(l)\n",
        "  return sentences_vectors\n",
        "\n",
        "\n",
        "def get_sum_sentence_vector_en(embeddings, line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    emb = get_word_vector_en(embeddings, w)\n",
        "    #do not add if the word is out of vocabulary\n",
        "    if emb is not None:\n",
        "      vectors.append(emb)\n",
        "  if vectors:\n",
        "    return torch.sum(torch.stack(vectors), dim=0).tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "# assume dim 100\n",
        "def get_sum_embeddings_en(f, embeddings, nlp):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "\n",
        "  for l in lines:\n",
        "    sentence = preprocess_en(l, nlp)\n",
        "    try:\n",
        "      vec = get_sum_sentence_vector_en(embeddings, sentence)\n",
        "      sentences_vectors.append(vec)\n",
        "    except:\n",
        "      sentences_vectors.append(0)\n",
        "\n",
        "  return sentences_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6XZmLepmQxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zh_train_mt_100_s = get_sum_embeddings_zh(\"./train.enzh.mt\", wv_from_bin_100)\n",
        "zh_train_src_100_s = get_sum_embeddings_en(\"./train.enzh.src\", glove, nlp_en)\n",
        "\n",
        "zh_val_mt_100_s = get_sum_embeddings_zh(\"./dev.enzh.mt\", wv_from_bin_100)\n",
        "zh_val_src_100_s = get_sum_embeddings_en(\"./dev.enzh.src\", glove, nlp_en)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8Rn3RqfmYum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_100_s = [x + y for x, y in zip(zh_train_src_100_s, zh_train_mt_100_s)]\n",
        "X_train_zh_100_s = np.array(X_train_100_s)\n",
        "\n",
        "X_val_100_s = [x + y for x, y in zip(zh_val_src_100_s, zh_val_mt_100_s)]\n",
        "X_val_zh_100_s = np.array(X_val_100_s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6PSaergmfjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_s, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_s)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "\"\"\"\n",
        "linear\n",
        "RMSE: 0.9203004562821944 Pearson 0.2525785904226626\n",
        "\n",
        "poly\n",
        "RMSE: 0.9499024011434094 Pearson 0.18792023737965777\n",
        "\n",
        "rbf\n",
        "RMSE: 0.905470160037738 Pearson 0.29378234780721474\n",
        "\n",
        "sigmoid\n",
        "RMSE: 34.73144893811673 Pearson -0.004944951711832229\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtlJk1Bz5jya",
        "colab_type": "text"
      },
      "source": [
        "### Using Min/Max of Word Embedding Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51ALxvSE5vHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_min_sentence_vector_zh(line, word_vectors):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    try:\n",
        "      emb = word_vectors[w]\n",
        "      vectors.append(emb)\n",
        "    except:\n",
        "      pass #Do not add if the word is out of vocabulary\n",
        "  if vectors:\n",
        "    return np.amin(vectors, axis=0).tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "def get_min_embeddings_zh(f, word_vectors):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "  for l in lines:\n",
        "    sent  = processing_zh(l)\n",
        "    vec = get_min_sentence_vector_zh(sent, word_vectors)\n",
        "\n",
        "    if vec is not None:\n",
        "      sentences_vectors.append(vec)\n",
        "    else:\n",
        "      print(l)\n",
        "  return sentences_vectors\n",
        "\n",
        "\n",
        "def get_min_sentence_vector_en(embeddings, line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    emb = get_word_vector_en(embeddings, w)\n",
        "    #do not add if the word is out of vocabulary\n",
        "    if emb is not None:\n",
        "      vectors.append(emb)\n",
        "  if vectors:\n",
        "    return torch.min(torch.stack(vectors), dim=0)[0].tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "# assume dim 100\n",
        "def get_min_embeddings_en(f, embeddings, nlp):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "\n",
        "  for l in lines:\n",
        "    sentence = preprocess_en(l, nlp)\n",
        "    try:\n",
        "      vec = get_min_sentence_vector_en(embeddings, sentence)\n",
        "      sentences_vectors.append(vec)\n",
        "    except:\n",
        "      sentences_vectors.append(0)\n",
        "\n",
        "  return sentences_vectors\n",
        "\n",
        "\n",
        "def get_max_sentence_vector_zh(line, word_vectors):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    try:\n",
        "      emb = word_vectors[w]\n",
        "      vectors.append(emb)\n",
        "    except:\n",
        "      pass #Do not add if the word is out of vocabulary\n",
        "  if vectors:\n",
        "    return np.amax(vectors, axis=0).tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "def get_max_embeddings_zh(f, word_vectors):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "  for l in lines:\n",
        "    sent  = processing_zh(l)\n",
        "    vec = get_max_sentence_vector_zh(sent, word_vectors)\n",
        "\n",
        "    if vec is not None:\n",
        "      sentences_vectors.append(vec)\n",
        "    else:\n",
        "      print(l)\n",
        "  return sentences_vectors\n",
        "\n",
        "\n",
        "def get_max_sentence_vector_en(embeddings, line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    emb = get_word_vector_en(embeddings, w)\n",
        "    #do not add if the word is out of vocabulary\n",
        "    if emb is not None:\n",
        "      vectors.append(emb)\n",
        "  if vectors:\n",
        "    return torch.max(torch.stack(vectors), dim=0)[0].tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "# assume dim 100\n",
        "def get_max_embeddings_en(f, embeddings, nlp):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "\n",
        "  for l in lines:\n",
        "    sentence = preprocess_en(l, nlp)\n",
        "    try:\n",
        "      vec = get_max_sentence_vector_en(embeddings, sentence)\n",
        "      sentences_vectors.append(vec)\n",
        "    except:\n",
        "      sentences_vectors.append(0)\n",
        "\n",
        "  return sentences_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDrkNPi97CP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zh_train_mt_100_min = get_min_embeddings_zh(\"./train.enzh.mt\", wv_from_bin_100)\n",
        "zh_train_src_100_min = get_min_embeddings_en(\"./train.enzh.src\", glove, nlp_en)\n",
        "zh_val_mt_100_min = get_min_embeddings_zh(\"./dev.enzh.mt\", wv_from_bin_100)\n",
        "zh_val_src_100_min = get_min_embeddings_en(\"./dev.enzh.src\", glove, nlp_en)\n",
        "\n",
        "zh_train_mt_100_max = get_max_embeddings_zh(\"./train.enzh.mt\", wv_from_bin_100)\n",
        "zh_train_src_100_max = get_max_embeddings_en(\"./train.enzh.src\", glove, nlp_en)\n",
        "zh_val_mt_100_max = get_max_embeddings_zh(\"./dev.enzh.mt\", wv_from_bin_100)\n",
        "zh_val_src_100_max = get_max_embeddings_en(\"./dev.enzh.src\", glove, nlp_en)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUCHBHh57UhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_100_min = [x + y for x, y in zip(zh_train_src_100_min, zh_train_mt_100_min)]\n",
        "X_train_zh_100_min = np.array(X_train_100_min)\n",
        "X_val_100_min = [x + y for x, y in zip(zh_val_src_100_min, zh_val_mt_100_min)]\n",
        "X_val_zh_100_min = np.array(X_val_100_min)\n",
        "\n",
        "X_train_100_max = [x + y for x, y in zip(zh_train_src_100_max, zh_train_mt_100_max)]\n",
        "X_train_zh_100_max = np.array(X_train_100_max)\n",
        "X_val_100_max = [x + y for x, y in zip(zh_val_src_100_max, zh_val_mt_100_max)]\n",
        "X_val_zh_100_max = np.array(X_val_100_max)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC-LkWyjAv9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"min\")\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_min, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_min)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "print(\"max\")\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_max, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_max)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "\"\"\"\n",
        "min\n",
        "linear\n",
        "RMSE: 0.9174155670157896 Pearson 0.26925288419123733\n",
        "\n",
        "poly\n",
        "RMSE: 0.9721279726125316 Pearson 0.23475167649201809\n",
        "\n",
        "rbf\n",
        "RMSE: 0.9049414520907609 Pearson 0.2920329610610498\n",
        "\n",
        "sigmoid\n",
        "RMSE: 26.865545536417198 Pearson 0.0076818897644271664\n",
        "\n",
        "max\n",
        "linear\n",
        "RMSE: 0.9288259124772581 Pearson 0.2342204449314311\n",
        "\n",
        "poly\n",
        "RMSE: 1.0188986421056523 Pearson 0.18458085384794573\n",
        "\n",
        "rbf\n",
        "RMSE: 0.9074016100647716 Pearson 0.28092258260753544\n",
        "\n",
        "sigmoid\n",
        "RMSE: 26.529780761913454 Pearson -0.007926148030142141\n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qThYLeO7oJPB",
        "colab_type": "text"
      },
      "source": [
        "### Combinations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkpY7XYnoVr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# min + max\n",
        "X_train_100_mm = [sum(t, []) for t in zip(zh_train_src_100_min, zh_train_src_100_max, zh_train_mt_100_min, zh_train_mt_100_max)]\n",
        "X_train_zh_100_mm = np.array(X_train_100_mm)\n",
        "X_val_100_mm = [sum(t, []) for t in zip(zh_val_src_100_min, zh_val_src_100_max, zh_val_mt_100_min, zh_val_mt_100_max)]\n",
        "X_val_zh_100_mm = np.array(X_val_100_mm)\n",
        "\n",
        "# min + avg + max\n",
        "X_train_100_mam = [sum(t, []) for t in zip(zh_train_src_100_min, zh_train_src_100_a, zh_train_src_100_max, zh_train_mt_100_min, zh_train_mt_100_a, zh_train_mt_100_max)]\n",
        "X_train_zh_100_mam = np.array(X_train_100_mam)\n",
        "X_val_100_mam = [sum(t, []) for t in zip(zh_val_src_100_min, zh_val_src_100_a, zh_val_src_100_max, zh_val_mt_100_min, zh_val_mt_100_a, zh_val_mt_100_max)]\n",
        "X_val_zh_100_mam = np.array(X_val_100_mam)\n",
        "\n",
        "# avg + sum\n",
        "X_train_100_as = [sum(t, []) for t in zip(zh_train_src_100_a, zh_train_src_100_s, zh_train_mt_100_a, zh_train_mt_100_s)]\n",
        "X_train_zh_100_as = np.array(X_train_100_mam)\n",
        "X_val_100_as = [sum(t, []) for t in zip(zh_val_src_100_a, zh_val_src_100_s, zh_val_mt_100_a, zh_val_mt_100_s)]\n",
        "X_val_zh_100_as = np.array(X_val_100_mam)\n",
        "\n",
        "# min + avg + max + sum\n",
        "X_train_100_mams = [sum(t, []) for t in zip(zh_train_src_100_min, zh_train_src_100_a, zh_train_src_100_max, zh_train_src_100_s, zh_train_mt_100_min, zh_train_mt_100_a, zh_train_mt_100_max, zh_train_src_100_s)]\n",
        "X_train_zh_100_mams = np.array(X_train_100_mam)\n",
        "X_val_100_mams = [sum(t, []) for t in zip(zh_val_src_100_min, zh_val_src_100_a, zh_val_src_100_max, zh_val_src_100_s, zh_val_mt_100_min, zh_val_mt_100_a, zh_val_mt_100_max, zh_val_mt_100_s)]\n",
        "X_val_zh_100_mams = np.array(X_val_100_mam)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMabHUiVo-o6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"min + max\")\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_mm, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_mm)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "print(\"min + avg + max\")\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_mam, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_mam)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "print(\"avg + sum\")\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_as, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_as)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "print(\"min + avg + max + sum\")\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_mams, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_mams)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "\"\"\"\n",
        "min + max\n",
        "linear\n",
        "RMSE: 0.9165262212864648 Pearson 0.26668369681069803\n",
        "\n",
        "poly\n",
        "RMSE: 0.9081786905379852 Pearson 0.3058041114768354\n",
        "\n",
        "rbf\n",
        "RMSE: 0.9084429289156349 Pearson 0.30464644770878846\n",
        "\n",
        "sigmoid\n",
        "RMSE: 3.5248833190245237 Pearson -0.006792767476776504\n",
        "\n",
        "min + avg + max\n",
        "linear\n",
        "RMSE: 0.9081244309157551 Pearson 0.28851234087420935\n",
        "\n",
        "poly\n",
        "RMSE: 0.8982943971746072 Pearson 0.3296975280939544\n",
        "\n",
        "rbf\n",
        "RMSE: 0.8992479975159785 Pearson 0.3292233306043897\n",
        "\n",
        "sigmoid\n",
        "RMSE: 2.600000956750181 Pearson 0.014142349847365135\n",
        "\n",
        "avg + sum\n",
        "linear\n",
        "RMSE: 0.9081244309157551 Pearson 0.28851234087420935\n",
        "\n",
        "poly\n",
        "RMSE: 0.8982943971746072 Pearson 0.3296975280939544\n",
        "\n",
        "rbf\n",
        "RMSE: 0.8992479975159785 Pearson 0.3292233306043897\n",
        "\n",
        "sigmoid\n",
        "RMSE: 2.600000956750181 Pearson 0.014142349847365135\n",
        "\n",
        "min + avg + max + sum\n",
        "linear\n",
        "RMSE: 0.9081244309157551 Pearson 0.28851234087420935\n",
        "\n",
        "poly\n",
        "RMSE: 0.8982943971746072 Pearson 0.3296975280939544\n",
        "\n",
        "rbf\n",
        "RMSE: 0.8992479975159785 Pearson 0.3292233306043897\n",
        "\n",
        "sigmoid\n",
        "RMSE: 2.600000956750181 Pearson 0.014142349847365135\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_eVxCFdo8kJ",
        "colab_type": "text"
      },
      "source": [
        "## Results\n",
        "\n",
        "(Haven't tested the function yet...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XimIq82so7Op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def writeScores(scores):\n",
        "    fn = \"predictions.txt\"\n",
        "    print(\"\")\n",
        "    with open(fn, 'w') as output_file:\n",
        "        for idx,x in enumerate(scores):\n",
        "            #out =  metrics[idx]+\":\"+str(\"{0:.2f}\".format(x))+\"\\n\"\n",
        "            #print(out)\n",
        "            output_file.write(f\"{x}\\n\")\n",
        "\n",
        "\n",
        "def downloadScores(method_name, scores):\n",
        "  writeScores(scores)\n",
        "  with ZipFile(f\"en-zh_{method_name}.zip\", \"w\") as newzip:\n",
        "    newzip.write(\"predictions.txt\")\n",
        "  \n",
        "  files.download(f\"en-zh_{method_name}.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "471QN-wLp1ER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}