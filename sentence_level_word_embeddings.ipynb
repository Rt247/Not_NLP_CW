{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentence-level-word-embeddings",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rt247/Not_NLP_CW/blob/sentence-level-word-embeddings/sentence_level_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6BshlcaZgmT",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Download datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzekR1EZY_5C",
        "colab_type": "code",
        "outputId": "ed5db0ce-008d-4b58-a4b3-7a214bdb0069",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "source": [
        "from os.path import exists\n",
        "\n",
        "if not exists('enzh_data.zip'):\n",
        "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
        "    !unzip enzh_data.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-16 16:59:44--  https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=d663b319d7e775e112f5d67347b3db2fd22c50012403672b5f64350e85785d50&X-Amz-Date=20200216T165944Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200216%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2020-02-16 16:59:45--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=d663b319d7e775e112f5d67347b3db2fd22c50012403672b5f64350e85785d50&X-Amz-Date=20200216T165944Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200216%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 870893 (850K) [application/zip]\n",
            "Saving to: ‘enzh_data.zip’\n",
            "\n",
            "enzh_data.zip       100%[===================>] 850.48K  1.00MB/s    in 0.8s    \n",
            "\n",
            "2020-02-16 16:59:47 (1.00 MB/s) - ‘enzh_data.zip’ saved [870893/870893]\n",
            "\n",
            "Archive:  enzh_data.zip\n",
            "  inflating: dev.enzh.mt             \n",
            "  inflating: dev.enzh.scores         \n",
            "  inflating: dev.enzh.src            \n",
            "  inflating: test.enzh.mt            \n",
            "  inflating: test.enzh.src           \n",
            "  inflating: train.enzh.mt           \n",
            "  inflating: train.enzh.src          \n",
            "  inflating: train.enzh.scores       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UskRgN-6ZoKo",
        "colab_type": "text"
      },
      "source": [
        "Check data downloaded successfully:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQUZyQrXY_5a",
        "colab_type": "code",
        "outputId": "78daba0f-77e9-432a-9920-d9d35591511f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "with open(\"./train.enzh.src\", \"r\") as enzh_src:\n",
        "  print(\"Source: \",enzh_src.readline())\n",
        "with open(\"./train.enzh.mt\", \"r\") as enzh_mt:\n",
        "  print(\"Translation: \",enzh_mt.readline())\n",
        "with open(\"./train.enzh.scores\", \"r\") as enzh_scores:\n",
        "  print(\"Score: \",enzh_scores.readline())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source:  The last conquistador then rides on with his sword drawn.\n",
            "\n",
            "Translation:  最后的征服者骑着他的剑继续前进.\n",
            "\n",
            "Score:  -1.5284005772625449\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV-JSE2sZzoT",
        "colab_type": "text"
      },
      "source": [
        "### English Models Setup\n",
        "\n",
        "Download English models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkP0aR4rZbVA",
        "colab_type": "code",
        "outputId": "92c0d8c1-e631-4626-9af0-a4700cb41934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "!spacy download en_core_web_md\n",
        "!spacy link en_core_web_md en300"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 1.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126236 sha256=dd39bff08fe2e375ee2d478c828fb2164237de33f0f5e10d96af2b45f9691fa5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lunku6yd/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en300\n",
            "You can now load the model via spacy.load('en300')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYByRCqQaQDH",
        "colab_type": "text"
      },
      "source": [
        "Load a GloVe English model with dim 100, and spaCy English model with dim 300.\n",
        "\n",
        "Some Chinese models only have **dim 100**, so we will need to **tokenize with spaCy, then embed with GloVe**.\n",
        "\n",
        "Other Chinese models have **dim 300**, so we can **just use spaCy**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVfblxyVaO3r",
        "colab_type": "code",
        "outputId": "993885f0-85f3-430f-b257-cc82a0c5568a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torchtext\n",
        "import spacy\n",
        "\n",
        "# Embedding for English when dim 100\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
        "\n",
        "# Tokenizer for English when dim 100, Tokenizer and Embedding when dim 300\n",
        "nlp_en = spacy.load('en300')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:26, 2.23MB/s]                          \n",
            "100%|█████████▉| 399919/400000 [00:33<00:00, 22662.15it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tWbs9PucbS0",
        "colab_type": "text"
      },
      "source": [
        "Functions for processing English dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoKMJIr5acPj",
        "colab_type": "code",
        "outputId": "6dac0315-64d1-43ee-d957-9f4a5d8ce910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#downloading stopwords from the nltk package\n",
        "download('stopwords') #stopwords dictionary, run once\n",
        "stop_words_en = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def preprocess_en(sentence, nlp):\n",
        "    text = sentence.lower()\n",
        "    doc = [token.lemma_ for token in  nlp.tokenizer(text)]\n",
        "    doc = [word for word in doc if word not in stop_words_en]\n",
        "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
        "    return doc\n",
        "\n",
        "def get_word_vector_en(embeddings, word):\n",
        "    try:\n",
        "      vec = embeddings.vectors[embeddings.stoi[word]]\n",
        "      return vec\n",
        "    except KeyError:\n",
        "      #print(f\"Word {word} does not exist\")\n",
        "      pass\n",
        "\n",
        "def get_sentence_vector_en(embeddings, line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    emb = get_word_vector_en(embeddings, w)\n",
        "    #do not add if the word is out of vocabulary\n",
        "    if emb is not None:\n",
        "      vectors.append(emb)\n",
        "   \n",
        "  return torch.mean(torch.stack(vectors))\n",
        "\n",
        "def get_sentence_emb_en(line, nlp):\n",
        "  text = line.lower()\n",
        "  l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
        "  l = ' '.join([word for word in l if word not in stop_words_en])\n",
        "\n",
        "  sen = nlp(l)\n",
        "  return sen.vector\n",
        "\n",
        "\n",
        "# By default we assume dim of Chinese model will be 100, and so we will need\n",
        "# to embed English model with dim 100.\n",
        "# If using Chinese model with dim 300, set dim=300.\n",
        "def get_embeddings_en(f, embeddings, nlp, dim=100):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "\n",
        "  if dim == 300:\n",
        "    for l in lines:\n",
        "      vec = get_sentence_emb_en(l, nlp)\n",
        "      if vec is not None:\n",
        "        vec = np.mean(vec)\n",
        "        sentences_vectors.append(vec)\n",
        "      else:\n",
        "        sentences_vectors.append(0)\n",
        "    return sentences_vectors\n",
        "\n",
        "  for l in lines:\n",
        "    sentence = preprocess_en(l, nlp)\n",
        "    try:\n",
        "      vec = get_sentence_vector_en(embeddings, sentence)\n",
        "      sentences_vectors.append(vec)\n",
        "    except:\n",
        "      sentences_vectors.append(0)\n",
        "\n",
        "  return sentences_vectors\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1930-s6fN7T",
        "colab_type": "text"
      },
      "source": [
        "### Chinese Models Setup\n",
        "\n",
        "Download Chinese stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O14b4JeHcNhB",
        "colab_type": "code",
        "outputId": "5027cc6c-11c0-4c27-d8a7-185ec5daca8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget -c https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-16 17:07:51--  https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘chinese_stop_words.txt’\n",
            "\n",
            "chinese_stop_words.     [  <=>               ] 416.74K  1.28MB/s    in 0.3s    \n",
            "\n",
            "2020-02-16 17:07:51 (1.28 MB/s) - ‘chinese_stop_words.txt’ saved [426741]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw437fJCip68",
        "colab_type": "text"
      },
      "source": [
        "Download and load Chinese model with **dim 100** (University of Oslo):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8V-nUwUikda",
        "colab_type": "code",
        "outputId": "083e0ab5-bf8d-45b6-90fd-b00b4d249c7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "!wget -O zh_100.zip http://vectors.nlpl.eu/repository/20/35.zip\n",
        "\n",
        "!unzip zh_100.zip -d ./zh_100\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "wv_from_bin_100 = KeyedVectors.load_word2vec_format(\"./zh_100/model.bin\", binary=True) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-16 17:07:53--  http://vectors.nlpl.eu/repository/20/35.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1458485917 (1.4G) [application/zip]\n",
            "Saving to: ‘zh_100.zip’\n",
            "\n",
            "zh_100.zip          100%[===================>]   1.36G  16.9MB/s    in 91s     \n",
            "\n",
            "2020-02-16 17:09:25 (15.2 MB/s) - ‘zh_100.zip’ saved [1458485917/1458485917]\n",
            "\n",
            "Archive:  zh_100.zip\n",
            "  inflating: ./zh_100/LIST           \n",
            "  inflating: ./zh_100/meta.json      \n",
            "  inflating: ./zh_100/model.bin      \n",
            "  inflating: ./zh_100/model.txt      \n",
            "  inflating: ./zh_100/README         \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_QhNHjwl0rz",
        "colab_type": "text"
      },
      "source": [
        "Download and load Chinese moel with **dim 300** (Kyubyong):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p8qkZi_ivhZ",
        "colab_type": "code",
        "outputId": "8adeeb9e-019e-4e80-f12f-741c0a3f8309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "!pip install gdown\n",
        "\n",
        "!gdown -O zh_300.zip https://drive.google.com/uc?id=0B0ZXk88koS2KNER5UHNDY19pbzQ\n",
        "\n",
        "!unzip zh_300.zip -d ./zh_300\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "wv_from_bin_300 = Word2Vec.load(\"./zh_300/zh.bin\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.8)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=0B0ZXk88koS2KNER5UHNDY19pbzQ\n",
            "To: /content/zh_300.zip\n",
            "203MB [00:01, 106MB/s] \n",
            "Archive:  zh_300.zip\n",
            "  inflating: ./zh_300/zh.bin         \n",
            "  inflating: ./zh_300/zh.tsv         \n",
            "  inflating: ./zh_300/zh.bin.syn1neg.npy  \n",
            "  inflating: ./zh_300/zh.bin.syn0.npy  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo5vhydafzC5",
        "colab_type": "text"
      },
      "source": [
        "Functions for processing Chinese dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s27T7LXlf4sG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import jieba\n",
        "import gensim \n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "stop_words = [ line.rstrip() for line in open('./chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
        "\n",
        "\n",
        "def get_sentence_vector_zh(line, word_vectors):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    try:\n",
        "      emb = word_vectors[w]\n",
        "      vectors.append(emb)\n",
        "    except:\n",
        "      pass #Do not add if the word is out of vocabulary\n",
        "  if vectors:\n",
        "    vectors = np.array(vectors)\n",
        "    return np.mean(vectors)  \n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "def processing_zh(sentence):\n",
        "  seg_list = jieba.lcut(sentence,cut_all=True)\n",
        "  doc = [word for word in seg_list if word not in stop_words]\n",
        "  docs = [e for e in doc if e.isalnum()]\n",
        "  return docs\n",
        "\n",
        "\n",
        "def get_embeddings_zh(f, word_vectors):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "  for l in lines:\n",
        "    sent  = processing_zh(l)\n",
        "    vec = get_sentence_vector_zh(sent, word_vectors)\n",
        "\n",
        "    if vec is not None:\n",
        "      sentences_vectors.append(vec)\n",
        "    else:\n",
        "      print(l)\n",
        "  return sentences_vectors\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI0BAckemQOU",
        "colab_type": "text"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ6nLoxEmWN-",
        "colab_type": "code",
        "outputId": "8f23d5a9-0007-4b61-89cd-c3ba6e121593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "import spacy\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "\n",
        "zh_train_mt_100 = get_embeddings_zh(\"./train.enzh.mt\", wv_from_bin_100)\n",
        "zh_train_mt_300 = get_embeddings_zh(\"./train.enzh.mt\", wv_from_bin_300)\n",
        "zh_train_src_100 = get_embeddings_en(\"./train.enzh.src\", glove, nlp_en, dim=100)\n",
        "zh_train_src_300 = get_embeddings_en(\"./train.enzh.src\", glove, nlp_en, dim=300)\n",
        "f_train_scores = open(\"./train.enzh.scores\", 'r')\n",
        "zh_train_scores = f_train_scores.readlines()\n",
        "\n",
        "zh_val_mt_100 = get_embeddings_zh(\"./dev.enzh.mt\", wv_from_bin_100)\n",
        "zh_val_mt_300 = get_embeddings_zh(\"./dev.enzh.mt\", wv_from_bin_300)\n",
        "zh_val_src_100 = get_embeddings_en(\"./dev.enzh.src\", glove, nlp_en, dim=100)\n",
        "zh_val_src_300 = get_embeddings_en(\"./dev.enzh.src\", glove, nlp_en, dim=300)\n",
        "f_val_scores = open(\"./dev.enzh.scores\", 'r')\n",
        "zh_val_scores = f_val_scores.readlines()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.879 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXyQxiYxnSTV",
        "colab_type": "text"
      },
      "source": [
        "Check embedded correctly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h_nqGmxnRBO",
        "colab_type": "code",
        "outputId": "6c0a7753-bcf7-4812-ad6c-9ed1b94937da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(f\"Training mt (100): {len(zh_train_mt_100)} Training mt (300): {len(zh_train_mt_300)} Training src (100): {len(zh_train_src_100)} Training src (300): {len(zh_train_src_300)}\")\n",
        "print()\n",
        "print(f\"Validation mt (100): {len(zh_val_mt_100)} Validation mt (300): {len(zh_val_mt_300)} Validation src (100): {len(zh_val_src_100)} Validation src (300): {len(zh_val_src_300)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training mt (100): 7000 Training mt (300): 7000 Training src (100): 7000 Training src (300): 7000\n",
            "\n",
            "Validation mt (100): 1000 Validation mt (300): 1000 Validation src (100): 1000 Validation src (300): 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg4YoR5eoZU3",
        "colab_type": "text"
      },
      "source": [
        "Setup input and predicted outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXRO34atmqiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X_train_100 = [np.array(zh_train_src_100), np.array(zh_train_mt_100)]\n",
        "X_train_zh_100 = np.array(X_train_100).transpose()\n",
        "\n",
        "X_val_100 = [np.array(zh_val_src_100),np.array(zh_val_mt_100)]\n",
        "X_val_zh_100 = np.array(X_val_100).transpose()\n",
        "\n",
        "X_train_300 = [np.array(zh_train_src_300), np.array(zh_train_mt_300)]\n",
        "X_train_zh_300 = np.array(X_train_300).transpose()\n",
        "\n",
        "X_val_300 = [np.array(zh_val_src_300),np.array(zh_val_mt_300)]\n",
        "X_val_zh_300 = np.array(X_val_300).transpose()\n",
        "\n",
        "#Scores\n",
        "train_scores = np.array(zh_train_scores).astype(float)\n",
        "y_train_zh = train_scores\n",
        "\n",
        "val_scores = np.array(zh_val_scores).astype(float)\n",
        "y_val_zh = val_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLV1j47Vo97i",
        "colab_type": "text"
      },
      "source": [
        "## Methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF-AQtKJ4_p_",
        "colab_type": "text"
      },
      "source": [
        "### Using Average Word Embedding Vectors\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpgfIUEhoOAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_avg_sentence_vector_zh(line, word_vectors):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    try:\n",
        "      emb = word_vectors[w]\n",
        "      vectors.append(emb)\n",
        "    except:\n",
        "      pass #Do not add if the word is out of vocabulary\n",
        "  if vectors:\n",
        "    return np.mean(vectors, axis=0).tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "def get_avg_embeddings_zh(f, word_vectors):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "  for l in lines:\n",
        "    sent  = processing_zh(l)\n",
        "    vec = get_avg_sentence_vector_zh(sent, word_vectors)\n",
        "\n",
        "    if vec is not None:\n",
        "      sentences_vectors.append(vec)\n",
        "    else:\n",
        "      print(l)\n",
        "  return sentences_vectors\n",
        "\n",
        "\n",
        "def get_avg_sentence_vector_en(embeddings, line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    emb = get_word_vector_en(embeddings, w)\n",
        "    #do not add if the word is out of vocabulary\n",
        "    if emb is not None:\n",
        "      vectors.append(emb)\n",
        "  if vectors:\n",
        "    return torch.mean(torch.stack(vectors), dim=0).tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "# assume dim 100\n",
        "def get_avg_embeddings_en(f, embeddings, nlp):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "\n",
        "  for l in lines:\n",
        "    sentence = preprocess_en(l, nlp)\n",
        "    try:\n",
        "      vec = get_avg_sentence_vector_en(embeddings, sentence)\n",
        "      sentences_vectors.append(vec)\n",
        "    except:\n",
        "      sentences_vectors.append(0)\n",
        "\n",
        "  return sentences_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO1pLTEQpNya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zh_train_mt_100_a = get_avg_embeddings_zh(\"./train.enzh.mt\", wv_from_bin_100)\n",
        "zh_train_src_100_a = get_avg_embeddings_en(\"./train.enzh.src\", glove, nlp_en)\n",
        "\n",
        "zh_val_mt_100_a = get_avg_embeddings_zh(\"./dev.enzh.mt\", wv_from_bin_100)\n",
        "zh_val_src_100_a = get_avg_embeddings_en(\"./dev.enzh.src\", glove, nlp_en)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJH3H9XqphpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_100_a = [x + y for x, y in zip(zh_train_src_100_a, zh_train_mt_100_a)]\n",
        "X_train_zh_100_a = np.array(X_train_100_a)\n",
        "\n",
        "X_val_100_a = [x + y for x, y in zip(zh_val_src_100_a, zh_val_mt_100_a)]\n",
        "X_val_zh_100_a = np.array(X_val_100_a)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY1auosCuegb",
        "colab_type": "code",
        "outputId": "6ba87ece-13fa-4c3f-8418-4a060a758b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from scipy.stats.stats import pearsonr\n",
        "\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_a, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_a)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear\n",
            "RMSE: 0.9044962563186333 Pearson 0.3017781690203462\n",
            "\n",
            "poly\n",
            "RMSE: 0.8990697909416231 Pearson 0.3032902746054339\n",
            "\n",
            "rbf\n",
            "RMSE: 0.8900985622788053 Pearson 0.3403404558003603\n",
            "\n",
            "sigmoid\n",
            "RMSE: 7.152607007355879 Pearson -0.03977439348067312\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvSlcMrH2tWv",
        "colab_type": "code",
        "outputId": "12ac6fd5-79a4-4bc0-c4fb-23457384741d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "# Import the model we are using\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators = 1000, random_state = 666)\n",
        "rf.fit(X_train_zh_100_a, y_train_zh);\n",
        "predictions = rf.predict(X_val_zh)\n",
        "\n",
        "pearson = pearsonr(y_val_zh_100_a, predictions)\n",
        "print('RMSE:', rmse(predictions,y_val_zh))\n",
        "print(f\"Pearson {pearson[0]}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c717b378803d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m666\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_zh_100_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_zh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_zh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_zh_100_f' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtlJk1Bz5jya",
        "colab_type": "text"
      },
      "source": [
        "### Using Min/Max of Word Embedding Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51ALxvSE5vHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_min_sentence_vector_zh(line, word_vectors):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    try:\n",
        "      emb = word_vectors[w]\n",
        "      vectors.append(emb)\n",
        "    except:\n",
        "      pass #Do not add if the word is out of vocabulary\n",
        "  if vectors:\n",
        "    return np.amin(vectors, axis=0).tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "def get_min_embeddings_zh(f, word_vectors):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "  for l in lines:\n",
        "    sent  = processing_zh(l)\n",
        "    vec = get_min_sentence_vector_zh(sent, word_vectors)\n",
        "\n",
        "    if vec is not None:\n",
        "      sentences_vectors.append(vec)\n",
        "    else:\n",
        "      print(l)\n",
        "  return sentences_vectors\n",
        "\n",
        "\n",
        "def get_min_sentence_vector_en(embeddings, line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    emb = get_word_vector_en(embeddings, w)\n",
        "    #do not add if the word is out of vocabulary\n",
        "    if emb is not None:\n",
        "      vectors.append(emb)\n",
        "  if vectors:\n",
        "    return torch.min(torch.stack(vectors), dim=0)[0].tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "# assume dim 100\n",
        "def get_min_embeddings_en(f, embeddings, nlp):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "\n",
        "  for l in lines:\n",
        "    sentence = preprocess_en(l, nlp)\n",
        "    try:\n",
        "      vec = get_min_sentence_vector_en(embeddings, sentence)\n",
        "      sentences_vectors.append(vec)\n",
        "    except:\n",
        "      sentences_vectors.append(0)\n",
        "\n",
        "  return sentences_vectors\n",
        "\n",
        "\n",
        "def get_max_sentence_vector_zh(line, word_vectors):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    try:\n",
        "      emb = word_vectors[w]\n",
        "      vectors.append(emb)\n",
        "    except:\n",
        "      pass #Do not add if the word is out of vocabulary\n",
        "  if vectors:\n",
        "    return np.amax(vectors, axis=0).tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "def get_max_embeddings_zh(f, word_vectors):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "  for l in lines:\n",
        "    sent  = processing_zh(l)\n",
        "    vec = get_max_sentence_vector_zh(sent, word_vectors)\n",
        "\n",
        "    if vec is not None:\n",
        "      sentences_vectors.append(vec)\n",
        "    else:\n",
        "      print(l)\n",
        "  return sentences_vectors\n",
        "\n",
        "\n",
        "def get_max_sentence_vector_en(embeddings, line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    emb = get_word_vector_en(embeddings, w)\n",
        "    #do not add if the word is out of vocabulary\n",
        "    if emb is not None:\n",
        "      vectors.append(emb)\n",
        "  if vectors:\n",
        "    return torch.max(torch.stack(vectors), dim=0)[0].tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "# assume dim 100\n",
        "def get_max_embeddings_en(f, embeddings, nlp):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "\n",
        "  for l in lines:\n",
        "    sentence = preprocess_en(l, nlp)\n",
        "    try:\n",
        "      vec = get_max_sentence_vector_en(embeddings, sentence)\n",
        "      sentences_vectors.append(vec)\n",
        "    except:\n",
        "      sentences_vectors.append(0)\n",
        "\n",
        "  return sentences_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDrkNPi97CP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zh_train_mt_100_min = get_min_embeddings_zh(\"./train.enzh.mt\", wv_from_bin_100)\n",
        "zh_train_src_100_min = get_min_embeddings_en(\"./train.enzh.src\", glove, nlp_en)\n",
        "zh_val_mt_100_min = get_min_embeddings_zh(\"./dev.enzh.mt\", wv_from_bin_100)\n",
        "zh_val_src_100_min = get_min_embeddings_en(\"./dev.enzh.src\", glove, nlp_en)\n",
        "\n",
        "zh_train_mt_100_max = get_max_embeddings_zh(\"./train.enzh.mt\", wv_from_bin_100)\n",
        "zh_train_src_100_max = get_max_embeddings_en(\"./train.enzh.src\", glove, nlp_en)\n",
        "zh_val_mt_100_max = get_max_embeddings_zh(\"./dev.enzh.mt\", wv_from_bin_100)\n",
        "zh_val_src_100_max = get_max_embeddings_en(\"./dev.enzh.src\", glove, nlp_en)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUCHBHh57UhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_100_min = [x + y for x, y in zip(zh_train_src_100_min, zh_train_mt_100_min)]\n",
        "X_train_zh_100_min = np.array(X_train_100_min)\n",
        "X_val_100_min = [x + y for x, y in zip(zh_val_src_100_min, zh_val_mt_100_min)]\n",
        "X_val_zh_100_min = np.array(X_val_100_min)\n",
        "\n",
        "X_train_100_max = [x + y for x, y in zip(zh_train_src_100_max, zh_train_mt_100_max)]\n",
        "X_train_zh_100_max = np.array(X_train_100_max)\n",
        "X_val_100_max = [x + y for x, y in zip(zh_val_src_100_max, zh_val_mt_100_max)]\n",
        "X_val_zh_100_max = np.array(X_val_100_max)\n",
        "\n",
        "X_train_100_mm = [x + y + z + w for x, y, z, w in zip(zh_train_src_100_min, zh_train_src_100_max, zh_train_mt_100_min, zh_train_mt_100_max)]\n",
        "X_train_zh_100_mm = np.array(X_train_100_mm)\n",
        "X_val_100_mm = [x + y + z + w for x, y, z, w in zip(zh_val_src_100_min, zh_val_src_100_max, zh_val_mt_100_min, zh_val_mt_100_max)]\n",
        "X_val_zh_100_mm = np.array(X_val_100_mm)\n",
        "\n",
        "X_train_100_mam = [sum(t, []) for t in zip(zh_train_src_100_min, zh_train_src_100_a, zh_train_src_100_max, zh_train_mt_100_min, zh_train_mt_100_a, zh_train_mt_100_max)]\n",
        "X_train_zh_100_mam = np.array(X_train_100_mam)\n",
        "X_val_100_mam = [sum(t, []) for t in zip(zh_val_src_100_min, zh_val_src_100_a, zh_val_src_100_max, zh_val_mt_100_min, zh_val_mt_100_a, zh_val_mt_100_max)]\n",
        "X_val_zh_100_mam = np.array(X_val_100_mam)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC-LkWyjAv9n",
        "colab_type": "code",
        "outputId": "9f3dd74a-de25-406f-e8e3-5a88e444d980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        }
      },
      "source": [
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_min, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_min)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_max, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_max)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_mm, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_mm)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_mam, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_mam)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear\n",
            "RMSE: 0.9174155670157896 Pearson 0.26925288419123733\n",
            "\n",
            "poly\n",
            "RMSE: 0.9721279726125316 Pearson 0.23475167649201809\n",
            "\n",
            "rbf\n",
            "RMSE: 0.9049414520907609 Pearson 0.2920329610610498\n",
            "\n",
            "sigmoid\n",
            "RMSE: 26.865545536417198 Pearson 0.0076818897644271664\n",
            "\n",
            "linear\n",
            "RMSE: 0.9288259124772581 Pearson 0.2342204449314311\n",
            "\n",
            "poly\n",
            "RMSE: 1.0188986421056523 Pearson 0.18458085384794573\n",
            "\n",
            "rbf\n",
            "RMSE: 0.9074016100647716 Pearson 0.28092258260753544\n",
            "\n",
            "sigmoid\n",
            "RMSE: 26.529780761913454 Pearson -0.007926148030142141\n",
            "\n",
            "linear\n",
            "RMSE: 0.9165262212864648 Pearson 0.26668369681069803\n",
            "\n",
            "poly\n",
            "RMSE: 0.9081786905379852 Pearson 0.3058041114768354\n",
            "\n",
            "rbf\n",
            "RMSE: 0.9084429289156349 Pearson 0.30464644770878846\n",
            "\n",
            "sigmoid\n",
            "RMSE: 3.5248833190245237 Pearson -0.006792767476776504\n",
            "\n",
            "linear\n",
            "RMSE: 0.9081244309157551 Pearson 0.28851234087420935\n",
            "\n",
            "poly\n",
            "RMSE: 0.8982943971746072 Pearson 0.3296975280939544\n",
            "\n",
            "rbf\n",
            "RMSE: 0.8992479975159785 Pearson 0.3292233306043897\n",
            "\n",
            "sigmoid\n",
            "RMSE: 2.600000956750181 Pearson 0.014142349847365135\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_eVxCFdo8kJ",
        "colab_type": "text"
      },
      "source": [
        "## Results\n",
        "\n",
        "(Haven't tested the function yet...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XimIq82so7Op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def writeScores(scores):\n",
        "    fn = \"predictions.txt\"\n",
        "    print(\"\")\n",
        "    with open(fn, 'w') as output_file:\n",
        "        for idx,x in enumerate(scores):\n",
        "            #out =  metrics[idx]+\":\"+str(\"{0:.2f}\".format(x))+\"\\n\"\n",
        "            #print(out)\n",
        "            output_file.write(f\"{x}\\n\")\n",
        "\n",
        "\n",
        "def downloadScores(method_name, scores):\n",
        "  writeScores(scores)\n",
        "  with ZipFile(f\"en-zh_{method_name}.zip\", \"w\") as newzip:\n",
        "    newzip.write(\"predictions.txt\")\n",
        "  \n",
        "  files.download(f\"en-zh_{method_name}.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "471QN-wLp1ER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}