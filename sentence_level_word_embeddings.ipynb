{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentence-level-word-embeddings",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rt247/Not_NLP_CW/blob/BERT_method/sentence_level_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6BshlcaZgmT",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Download datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzekR1EZY_5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os.path import exists\n",
        "\n",
        "if not exists('enzh_data.zip'):\n",
        "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
        "    !unzip enzh_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UskRgN-6ZoKo",
        "colab_type": "text"
      },
      "source": [
        "Check data downloaded successfully:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQUZyQrXY_5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"./train.enzh.src\", \"r\") as enzh_src:\n",
        "  print(\"Source: \",enzh_src.readline())\n",
        "with open(\"./train.enzh.mt\", \"r\") as enzh_mt:\n",
        "  print(\"Translation: \",enzh_mt.readline())\n",
        "with open(\"./train.enzh.scores\", \"r\") as enzh_scores:\n",
        "  print(\"Score: \",enzh_scores.readline())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV-JSE2sZzoT",
        "colab_type": "text"
      },
      "source": [
        "### English Models Setup\n",
        "\n",
        "Download English models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkP0aR4rZbVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!spacy download en_core_web_md\n",
        "!spacy link en_core_web_md en300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYByRCqQaQDH",
        "colab_type": "text"
      },
      "source": [
        "Load a GloVe English model with dim 100.\n",
        "\n",
        "Some Chinese models only have **dim 100**, so we will need to **tokenize with spaCy, then embed with GloVe**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVfblxyVaO3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "import spacy\n",
        "\n",
        "# Embedding for English when dim 100\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
        "\n",
        "# Tokenizer for English when dim 100, Tokenizer and Embedding when dim 300\n",
        "nlp_en = spacy.load('en300')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tWbs9PucbS0",
        "colab_type": "text"
      },
      "source": [
        "Functions for processing English dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoKMJIr5acPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#downloading stopwords from the nltk package\n",
        "download('stopwords') #stopwords dictionary, run once\n",
        "stop_words_en = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def preprocess_en(sentence, nlp):\n",
        "    text = sentence.lower()\n",
        "    doc = [token.lemma_ for token in  nlp.tokenizer(text)]\n",
        "    doc = [word for word in doc if word not in stop_words_en]\n",
        "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
        "    return doc\n",
        "\n",
        "def get_word_vector_en(embeddings, word):\n",
        "    try:\n",
        "      vec = embeddings.vectors[embeddings.stoi[word]]\n",
        "      return vec\n",
        "    except KeyError:\n",
        "      #print(f\"Word {word} does not exist\")\n",
        "      pass\n",
        "      \n",
        "\n",
        "def get_sentence_emb_en(line, nlp):\n",
        "  text = line.lower()\n",
        "  l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
        "  l = ' '.join([word for word in l if word not in stop_words_en])\n",
        "\n",
        "  sen = nlp(l)\n",
        "  return sen.vector\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1930-s6fN7T",
        "colab_type": "text"
      },
      "source": [
        "### Chinese Models Setup\n",
        "\n",
        "Download Chinese stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O14b4JeHcNhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -c https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw437fJCip68",
        "colab_type": "text"
      },
      "source": [
        "Download and load Chinese model with **dim 100** (University of Oslo):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8V-nUwUikda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not exists(\"zh_100.zip\"):\n",
        "  !wget -O zh_100.zip http://vectors.nlpl.eu/repository/20/35.zip\n",
        "  !unzip zh_100.zip -d ./zh_100\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "wv_from_bin_100 = KeyedVectors.load_word2vec_format(\"./zh_100/model.bin\", binary=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo5vhydafzC5",
        "colab_type": "text"
      },
      "source": [
        "Functions for processing Chinese dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s27T7LXlf4sG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import jieba\n",
        "import gensim \n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "stop_words = [ line.rstrip() for line in open('./chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
        "\n",
        "def processing_zh(sentence):\n",
        "  seg_list = jieba.lcut(sentence,cut_all=True)\n",
        "  doc = [word for word in seg_list if word not in stop_words]\n",
        "  docs = [e for e in doc if e.isalnum()]\n",
        "  return docs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1zT8sEaqJaQ",
        "colab_type": "text"
      },
      "source": [
        "### BERT embedding Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xA7VWIl2pDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertModel, AdamW, BertConfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3GCojqO4Qjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_oFKMjNqRa0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Scores\n",
        "f_train_scores = open(\"./train.enzh.scores\", 'r')\n",
        "zh_train_scores = f_train_scores.readlines()\n",
        "labels = np.array(zh_train_scores).astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Px5kVjo9nnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERILcV4iqYlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def token_raw_inputs(original, translated):\n",
        "  # Load pre-trained model tokenizer (vocabulary)\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "  text_pairs = list(zip(original, translated))\n",
        "  MAX_LENGTH = 128\n",
        "\n",
        "  inputs = [tokenizer.encode_plus(original, text_pair=translated, add_special_tokens = True, max_length=MAX_LENGTH, pad_to_max_length=True) for original, translated in text_pairs]\n",
        "\n",
        "  return [d['input_ids'] for d in inputs], [d['attention_mask'] for d in inputs]\n",
        "\n",
        "\n",
        "def get_BERT_embedding(input_tokens, attention_masks):\n",
        "  input_tensors = torch.tensor(input_tokens).to(device)\n",
        "  attention_mask_tensors = torch.tensor(attention_masks).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(input_tensors, attention_mask=attention_mask_tensors)\n",
        "  return last_hidden_states[0][:,0,:].cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI0BAckemQOU",
        "colab_type": "text"
      },
      "source": [
        "## Process Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ6nLoxEmWN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "\n",
        "f_train_scores = open(\"./train.enzh.scores\", 'r')\n",
        "zh_train_scores = f_train_scores.readlines()\n",
        "\n",
        "f_val_scores = open(\"./dev.enzh.scores\", 'r')\n",
        "zh_val_scores = f_val_scores.readlines()\n",
        "\n",
        "train_scores = np.array(zh_train_scores).astype(float)\n",
        "y_train_zh = train_scores\n",
        "\n",
        "val_scores = np.array(zh_val_scores).astype(float)\n",
        "y_val_zh = val_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLV1j47Vo97i",
        "colab_type": "text"
      },
      "source": [
        "## Support Vector Machines\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE4p089dsp1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup\n",
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from scipy.stats.stats import pearsonr\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IF-AQtKJ4_p_",
        "colab_type": "text"
      },
      "source": [
        "### Using Average Word Embedding Vectors\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpgfIUEhoOAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_avg_sentence_vector_zh(line, word_vectors):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    try:\n",
        "      emb = word_vectors[w]\n",
        "      vectors.append(emb)\n",
        "    except:\n",
        "      pass #Do not add if the word is out of vocabulary\n",
        "  if vectors:\n",
        "    return np.mean(vectors, axis=0).tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "def get_avg_embeddings_zh(f, word_vectors):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "  for l in lines:\n",
        "    sent  = processing_zh(l)\n",
        "    vec = get_avg_sentence_vector_zh(sent, word_vectors)\n",
        "\n",
        "    if vec is not None:\n",
        "      sentences_vectors.append(vec)\n",
        "    else:\n",
        "      print(l)\n",
        "  return sentences_vectors\n",
        "\n",
        "\n",
        "def get_avg_sentence_vector_en(embeddings, line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    emb = get_word_vector_en(embeddings, w)\n",
        "    #do not add if the word is out of vocabulary\n",
        "    if emb is not None:\n",
        "      vectors.append(emb)\n",
        "  if vectors:\n",
        "    return torch.mean(torch.stack(vectors), dim=0).tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "# assume dim 100\n",
        "def get_avg_embeddings_en(f, embeddings, nlp):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "\n",
        "  for l in lines:\n",
        "    sentence = preprocess_en(l, nlp)\n",
        "    try:\n",
        "      vec = get_avg_sentence_vector_en(embeddings, sentence)\n",
        "      sentences_vectors.append(vec)\n",
        "    except:\n",
        "      sentences_vectors.append(0)\n",
        "\n",
        "  return sentences_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO1pLTEQpNya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zh_train_mt_100_a = get_avg_embeddings_zh(\"./train.enzh.mt\", wv_from_bin_100)\n",
        "zh_train_src_100_a = get_avg_embeddings_en(\"./train.enzh.src\", glove, nlp_en)\n",
        "\n",
        "zh_val_mt_100_a = get_avg_embeddings_zh(\"./dev.enzh.mt\", wv_from_bin_100)\n",
        "zh_val_src_100_a = get_avg_embeddings_en(\"./dev.enzh.src\", glove, nlp_en)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJH3H9XqphpQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_100_a = [x + y for x, y in zip(zh_train_src_100_a, zh_train_mt_100_a)]\n",
        "X_train_zh_100_a = np.array(X_train_100_a)\n",
        "\n",
        "X_val_100_a = [x + y for x, y in zip(zh_val_src_100_a, zh_val_mt_100_a)]\n",
        "X_val_zh_100_a = np.array(X_val_100_a)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY1auosCuegb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_a, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_a)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "\"\"\"\n",
        "linear\n",
        "RMSE: 0.9044962563186333 Pearson 0.3017781690203462\n",
        "\n",
        "poly\n",
        "RMSE: 0.8990697909416231 Pearson 0.3032902746054339\n",
        "\n",
        "rbf\n",
        "RMSE: 0.8900985622788053 Pearson 0.3403404558003603\n",
        "\n",
        "sigmoid\n",
        "RMSE: 7.152607007355879 Pearson -0.03977439348067312\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nHytkDwlz3z",
        "colab_type": "text"
      },
      "source": [
        "### Using Sum of Word Embedding Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_3IKDI-l2iC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sum_sentence_vector_zh(line, word_vectors):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    try:\n",
        "      emb = word_vectors[w]\n",
        "      vectors.append(emb)\n",
        "    except:\n",
        "      pass #Do not add if the word is out of vocabulary\n",
        "  if vectors:\n",
        "    return np.sum(vectors, axis=0).tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "def get_sum_embeddings_zh(f, word_vectors):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "  for l in lines:\n",
        "    sent  = processing_zh(l)\n",
        "    vec = get_sum_sentence_vector_zh(sent, word_vectors)\n",
        "\n",
        "    if vec is not None:\n",
        "      sentences_vectors.append(vec)\n",
        "    else:\n",
        "      print(l)\n",
        "  return sentences_vectors\n",
        "\n",
        "\n",
        "def get_sum_sentence_vector_en(embeddings, line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    emb = get_word_vector_en(embeddings, w)\n",
        "    #do not add if the word is out of vocabulary\n",
        "    if emb is not None:\n",
        "      vectors.append(emb)\n",
        "  if vectors:\n",
        "    return torch.sum(torch.stack(vectors), dim=0).tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "# assume dim 100\n",
        "def get_sum_embeddings_en(f, embeddings, nlp):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "\n",
        "  for l in lines:\n",
        "    sentence = preprocess_en(l, nlp)\n",
        "    try:\n",
        "      vec = get_sum_sentence_vector_en(embeddings, sentence)\n",
        "      sentences_vectors.append(vec)\n",
        "    except:\n",
        "      sentences_vectors.append(0)\n",
        "\n",
        "  return sentences_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6XZmLepmQxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zh_train_mt_100_s = get_sum_embeddings_zh(\"./train.enzh.mt\", wv_from_bin_100)\n",
        "zh_train_src_100_s = get_sum_embeddings_en(\"./train.enzh.src\", glove, nlp_en)\n",
        "\n",
        "zh_val_mt_100_s = get_sum_embeddings_zh(\"./dev.enzh.mt\", wv_from_bin_100)\n",
        "zh_val_src_100_s = get_sum_embeddings_en(\"./dev.enzh.src\", glove, nlp_en)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8Rn3RqfmYum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_100_s = [x + y for x, y in zip(zh_train_src_100_s, zh_train_mt_100_s)]\n",
        "X_train_zh_100_s = np.array(X_train_100_s)\n",
        "\n",
        "X_val_100_s = [x + y for x, y in zip(zh_val_src_100_s, zh_val_mt_100_s)]\n",
        "X_val_zh_100_s = np.array(X_val_100_s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6PSaergmfjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_s, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_s)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "\"\"\"\n",
        "linear\n",
        "RMSE: 0.9203004562821944 Pearson 0.2525785904226626\n",
        "\n",
        "poly\n",
        "RMSE: 0.9499024011434094 Pearson 0.18792023737965777\n",
        "\n",
        "rbf\n",
        "RMSE: 0.905470160037738 Pearson 0.29378234780721474\n",
        "\n",
        "sigmoid\n",
        "RMSE: 34.73144893811673 Pearson -0.004944951711832229\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtlJk1Bz5jya",
        "colab_type": "text"
      },
      "source": [
        "### Using Min/Max of Word Embedding Vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51ALxvSE5vHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_min_sentence_vector_zh(line, word_vectors):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    try:\n",
        "      emb = word_vectors[w]\n",
        "      vectors.append(emb)\n",
        "    except:\n",
        "      pass #Do not add if the word is out of vocabulary\n",
        "  if vectors:\n",
        "    return np.amin(vectors, axis=0).tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "def get_min_embeddings_zh(f, word_vectors):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "  for l in lines:\n",
        "    sent  = processing_zh(l)\n",
        "    vec = get_min_sentence_vector_zh(sent, word_vectors)\n",
        "\n",
        "    if vec is not None:\n",
        "      sentences_vectors.append(vec)\n",
        "    else:\n",
        "      print(l)\n",
        "  return sentences_vectors\n",
        "\n",
        "\n",
        "def get_min_sentence_vector_en(embeddings, line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    emb = get_word_vector_en(embeddings, w)\n",
        "    #do not add if the word is out of vocabulary\n",
        "    if emb is not None:\n",
        "      vectors.append(emb)\n",
        "  if vectors:\n",
        "    return torch.min(torch.stack(vectors), dim=0)[0].tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "# assume dim 100\n",
        "def get_min_embeddings_en(f, embeddings, nlp):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "\n",
        "  for l in lines:\n",
        "    sentence = preprocess_en(l, nlp)\n",
        "    try:\n",
        "      vec = get_min_sentence_vector_en(embeddings, sentence)\n",
        "      sentences_vectors.append(vec)\n",
        "    except:\n",
        "      sentences_vectors.append(0)\n",
        "\n",
        "  return sentences_vectors\n",
        "\n",
        "\n",
        "def get_max_sentence_vector_zh(line, word_vectors):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    try:\n",
        "      emb = word_vectors[w]\n",
        "      vectors.append(emb)\n",
        "    except:\n",
        "      pass #Do not add if the word is out of vocabulary\n",
        "  if vectors:\n",
        "    return np.amax(vectors, axis=0).tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "def get_max_embeddings_zh(f, word_vectors):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "  for l in lines:\n",
        "    sent  = processing_zh(l)\n",
        "    vec = get_max_sentence_vector_zh(sent, word_vectors)\n",
        "\n",
        "    if vec is not None:\n",
        "      sentences_vectors.append(vec)\n",
        "    else:\n",
        "      print(l)\n",
        "  return sentences_vectors\n",
        "\n",
        "\n",
        "def get_max_sentence_vector_en(embeddings, line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    emb = get_word_vector_en(embeddings, w)\n",
        "    #do not add if the word is out of vocabulary\n",
        "    if emb is not None:\n",
        "      vectors.append(emb)\n",
        "  if vectors:\n",
        "    return torch.max(torch.stack(vectors), dim=0)[0].tolist()\n",
        "  return [0] * 100\n",
        "\n",
        "# assume dim 100\n",
        "def get_max_embeddings_en(f, embeddings, nlp):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "\n",
        "  for l in lines:\n",
        "    sentence = preprocess_en(l, nlp)\n",
        "    try:\n",
        "      vec = get_max_sentence_vector_en(embeddings, sentence)\n",
        "      sentences_vectors.append(vec)\n",
        "    except:\n",
        "      sentences_vectors.append(0)\n",
        "\n",
        "  return sentences_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDrkNPi97CP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zh_train_mt_100_min = get_min_embeddings_zh(\"./train.enzh.mt\", wv_from_bin_100)\n",
        "zh_train_src_100_min = get_min_embeddings_en(\"./train.enzh.src\", glove, nlp_en)\n",
        "zh_val_mt_100_min = get_min_embeddings_zh(\"./dev.enzh.mt\", wv_from_bin_100)\n",
        "zh_val_src_100_min = get_min_embeddings_en(\"./dev.enzh.src\", glove, nlp_en)\n",
        "\n",
        "zh_train_mt_100_max = get_max_embeddings_zh(\"./train.enzh.mt\", wv_from_bin_100)\n",
        "zh_train_src_100_max = get_max_embeddings_en(\"./train.enzh.src\", glove, nlp_en)\n",
        "zh_val_mt_100_max = get_max_embeddings_zh(\"./dev.enzh.mt\", wv_from_bin_100)\n",
        "zh_val_src_100_max = get_max_embeddings_en(\"./dev.enzh.src\", glove, nlp_en)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUCHBHh57UhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_100_min = [x + y for x, y in zip(zh_train_src_100_min, zh_train_mt_100_min)]\n",
        "X_train_zh_100_min = np.array(X_train_100_min)\n",
        "X_val_100_min = [x + y for x, y in zip(zh_val_src_100_min, zh_val_mt_100_min)]\n",
        "X_val_zh_100_min = np.array(X_val_100_min)\n",
        "\n",
        "X_train_100_max = [x + y for x, y in zip(zh_train_src_100_max, zh_train_mt_100_max)]\n",
        "X_train_zh_100_max = np.array(X_train_100_max)\n",
        "X_val_100_max = [x + y for x, y in zip(zh_val_src_100_max, zh_val_mt_100_max)]\n",
        "X_val_zh_100_max = np.array(X_val_100_max)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CC-LkWyjAv9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"min\")\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_min, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_min)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "print(\"max\")\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_max, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_max)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "\"\"\"\n",
        "min\n",
        "linear\n",
        "RMSE: 0.9174155670157896 Pearson 0.26925288419123733\n",
        "\n",
        "poly\n",
        "RMSE: 0.9721279726125316 Pearson 0.23475167649201809\n",
        "\n",
        "rbf\n",
        "RMSE: 0.9049414520907609 Pearson 0.2920329610610498\n",
        "\n",
        "sigmoid\n",
        "RMSE: 26.865545536417198 Pearson 0.0076818897644271664\n",
        "\n",
        "max\n",
        "linear\n",
        "RMSE: 0.9288259124772581 Pearson 0.2342204449314311\n",
        "\n",
        "poly\n",
        "RMSE: 1.0188986421056523 Pearson 0.18458085384794573\n",
        "\n",
        "rbf\n",
        "RMSE: 0.9074016100647716 Pearson 0.28092258260753544\n",
        "\n",
        "sigmoid\n",
        "RMSE: 26.529780761913454 Pearson -0.007926148030142141\n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qThYLeO7oJPB",
        "colab_type": "text"
      },
      "source": [
        "### Combinations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkpY7XYnoVr8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# min + max\n",
        "X_train_100_mm = [sum(t, []) for t in zip(zh_train_src_100_min, zh_train_src_100_max, zh_train_mt_100_min, zh_train_mt_100_max)]\n",
        "X_train_zh_100_mm = np.array(X_train_100_mm)\n",
        "X_val_100_mm = [sum(t, []) for t in zip(zh_val_src_100_min, zh_val_src_100_max, zh_val_mt_100_min, zh_val_mt_100_max)]\n",
        "X_val_zh_100_mm = np.array(X_val_100_mm)\n",
        "\n",
        "# min + avg + max\n",
        "X_train_100_mam = [sum(t, []) for t in zip(zh_train_src_100_min, zh_train_src_100_a, zh_train_src_100_max, zh_train_mt_100_min, zh_train_mt_100_a, zh_train_mt_100_max)]\n",
        "X_train_zh_100_mam = np.array(X_train_100_mam)\n",
        "X_val_100_mam = [sum(t, []) for t in zip(zh_val_src_100_min, zh_val_src_100_a, zh_val_src_100_max, zh_val_mt_100_min, zh_val_mt_100_a, zh_val_mt_100_max)]\n",
        "X_val_zh_100_mam = np.array(X_val_100_mam)\n",
        "\n",
        "# avg + sum\n",
        "X_train_100_as = [sum(t, []) for t in zip(zh_train_src_100_a, zh_train_src_100_s, zh_train_mt_100_a, zh_train_mt_100_s)]\n",
        "X_train_zh_100_as = np.array(X_train_100_mam)\n",
        "X_val_100_as = [sum(t, []) for t in zip(zh_val_src_100_a, zh_val_src_100_s, zh_val_mt_100_a, zh_val_mt_100_s)]\n",
        "X_val_zh_100_as = np.array(X_val_100_mam)\n",
        "\n",
        "# min + avg + max + sum\n",
        "X_train_100_mams = [sum(t, []) for t in zip(zh_train_src_100_min, zh_train_src_100_a, zh_train_src_100_max, zh_train_src_100_s, zh_train_mt_100_min, zh_train_mt_100_a, zh_train_mt_100_max, zh_train_src_100_s)]\n",
        "X_train_zh_100_mams = np.array(X_train_100_mam)\n",
        "X_val_100_mams = [sum(t, []) for t in zip(zh_val_src_100_min, zh_val_src_100_a, zh_val_src_100_max, zh_val_src_100_s, zh_val_mt_100_min, zh_val_mt_100_a, zh_val_mt_100_max, zh_val_mt_100_s)]\n",
        "X_val_zh_100_mams = np.array(X_val_100_mam)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMabHUiVo-o6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"min + max\")\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_mm, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_mm)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "print(\"min + avg + max\")\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_mam, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_mam)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "print(\"avg + sum\")\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_as, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_as)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "print(\"min + avg + max + sum\")\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh_100_mams, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh_100_mams)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "\"\"\"\n",
        "min + max\n",
        "linear\n",
        "RMSE: 0.9165262212864648 Pearson 0.26668369681069803\n",
        "\n",
        "poly\n",
        "RMSE: 0.9081786905379852 Pearson 0.3058041114768354\n",
        "\n",
        "rbf\n",
        "RMSE: 0.9084429289156349 Pearson 0.30464644770878846\n",
        "\n",
        "sigmoid\n",
        "RMSE: 3.5248833190245237 Pearson -0.006792767476776504\n",
        "\n",
        "min + avg + max\n",
        "linear\n",
        "RMSE: 0.9081244309157551 Pearson 0.28851234087420935\n",
        "\n",
        "poly\n",
        "RMSE: 0.8982943971746072 Pearson 0.3296975280939544\n",
        "\n",
        "rbf\n",
        "RMSE: 0.8992479975159785 Pearson 0.3292233306043897\n",
        "\n",
        "sigmoid\n",
        "RMSE: 2.600000956750181 Pearson 0.014142349847365135\n",
        "\n",
        "avg + sum\n",
        "linear\n",
        "RMSE: 0.9081244309157551 Pearson 0.28851234087420935\n",
        "\n",
        "poly\n",
        "RMSE: 0.8982943971746072 Pearson 0.3296975280939544\n",
        "\n",
        "rbf\n",
        "RMSE: 0.8992479975159785 Pearson 0.3292233306043897\n",
        "\n",
        "sigmoid\n",
        "RMSE: 2.600000956750181 Pearson 0.014142349847365135\n",
        "\n",
        "min + avg + max + sum\n",
        "linear\n",
        "RMSE: 0.9081244309157551 Pearson 0.28851234087420935\n",
        "\n",
        "poly\n",
        "RMSE: 0.8982943971746072 Pearson 0.3296975280939544\n",
        "\n",
        "rbf\n",
        "RMSE: 0.8992479975159785 Pearson 0.3292233306043897\n",
        "\n",
        "sigmoid\n",
        "RMSE: 2.600000956750181 Pearson 0.014142349847365135\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucAEkZByAVuw",
        "colab_type": "text"
      },
      "source": [
        "# BERT embedding with regression model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETaWnhasHnIy",
        "colab_type": "text"
      },
      "source": [
        "## Feature extraction using BERT\n",
        "\n",
        "Use BERT to generate sentence level embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU9m8A_wAk10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "original_texts = open(\"./train.enzh.src\").readlines()\n",
        "translated_texts = open(\"./train.enzh.mt\").readlines()\n",
        "input_ids, input_attention_masks = token_raw_inputs(original_texts, translated_texts)\n",
        "\n",
        "train_features = get_BERT_embedding(input_ids[0:1000], input_attention_masks[0:1000])\n",
        "for i in range(1, 7):\n",
        "  features = get_BERT_embedding(input_ids[i*1000:(i + 1)*1000], input_attention_masks[i*1000:(i + 1)*1000])\n",
        "  train_features = np.concatenate((train_features, features))\n",
        "train_labels = labels\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "dev_original_texts = open(\"./dev.enzh.src\").readlines()\n",
        "dev_translated_texts = open(\"./dev.enzh.mt\").readlines()\n",
        "\n",
        "zh_val_scores = open(\"./dev.enzh.scores\", 'r').readlines()\n",
        "test_labels = np.array(zh_val_scores).astype(float)\n",
        "\n",
        "test_input_ids, test_attention_masks = token_raw_inputs(dev_original_texts, dev_translated_texts)\n",
        "test_features = get_BERT_embedding(test_input_ids, test_attention_masks)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7JgQtTyH0Ih",
        "colab_type": "text"
      },
      "source": [
        "## Ridge Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8O_2FYKFr09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf = Ridge(alpha=2)\n",
        "clf.fit(train_features, train_labels)\n",
        "\n",
        "predictions = clf.predict(test_features)\n",
        "\n",
        "pearson = pearsonr(test_labels, predictions)\n",
        "print(f'RMSE: {rmse(predictions, test_labels)} Pearson {pearson[0]}')\n",
        "'RMSE: 0.861979277558717 Pearson 0.3621685325738159'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqbXdFEAH9nP",
        "colab_type": "text"
      },
      "source": [
        "## SVR with different kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qmRBC5xICyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(train_features, train_labels)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(test_features)\n",
        "    pearson = pearsonr(test_labels, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,test_labels)} Pearson {pearson[0]}')\n",
        "    print()\n",
        "\n",
        "\n",
        "'''linear\n",
        "RMSE: 0.8847769222998597 Pearson 0.3541554106726864\n",
        "\n",
        "poly\n",
        "RMSE: 0.8800424227174211 Pearson 0.3812763230624585\n",
        "\n",
        "rbf\n",
        "RMSE: 0.8846797573743066 Pearson 0.3739785444174556\n",
        "\n",
        "sigmoid\n",
        "RMSE: 0.9047304999833146 Pearson 0.3336589772215751'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_eVxCFdo8kJ",
        "colab_type": "text"
      },
      "source": [
        "# Results\n",
        "\n",
        "(Haven't tested the function yet...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XimIq82so7Op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def writeScores(scores):\n",
        "    fn = \"predictions.txt\"\n",
        "    print(\"\")\n",
        "    with open(fn, 'w') as output_file:\n",
        "        for idx,x in enumerate(scores):\n",
        "            #out =  metrics[idx]+\":\"+str(\"{0:.2f}\".format(x))+\"\\n\"\n",
        "            #print(out)\n",
        "            output_file.write(f\"{x}\\n\")\n",
        "\n",
        "\n",
        "def downloadScores(method_name, scores):\n",
        "  writeScores(scores)\n",
        "  with ZipFile(f\"en-zh_{method_name}.zip\", \"w\") as newzip:\n",
        "    newzip.write(\"predictions.txt\")\n",
        "  \n",
        "  files.download(f\"en-zh_{method_name}.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "471QN-wLp1ER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}