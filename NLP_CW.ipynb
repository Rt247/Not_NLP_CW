{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_CW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rt247/Not_NLP_CW/blob/master/NLP_CW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6BshlcaZgmT",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Download datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzekR1EZY_5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "outputId": "31754176-ad95-4bca-d1d8-02b2b3828ba1"
      },
      "source": [
        "from os.path import exists\n",
        "\n",
        "if not exists('enzh_data.zip'):\n",
        "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
        "    !unzip enzh_data.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-12 13:01:17--  https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=eb5cc2a37cd3e995902d43749373071da8686c4fbe0b0ff2a4fffb83b3ad712f&X-Amz-Date=20200212T130118Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200212%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2020-02-12 13:01:18--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=eb5cc2a37cd3e995902d43749373071da8686c4fbe0b0ff2a4fffb83b3ad712f&X-Amz-Date=20200212T130118Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200212%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 870893 (850K) [application/zip]\n",
            "Saving to: ‘enzh_data.zip’\n",
            "\n",
            "enzh_data.zip       100%[===================>] 850.48K  1001KB/s    in 0.8s    \n",
            "\n",
            "2020-02-12 13:01:21 (1001 KB/s) - ‘enzh_data.zip’ saved [870893/870893]\n",
            "\n",
            "Archive:  enzh_data.zip\n",
            "  inflating: dev.enzh.mt             \n",
            "  inflating: dev.enzh.scores         \n",
            "  inflating: dev.enzh.src            \n",
            "  inflating: test.enzh.mt            \n",
            "  inflating: test.enzh.src           \n",
            "  inflating: train.enzh.mt           \n",
            "  inflating: train.enzh.src          \n",
            "  inflating: train.enzh.scores       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UskRgN-6ZoKo",
        "colab_type": "text"
      },
      "source": [
        "Check data downloaded successfully:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQUZyQrXY_5a",
        "colab_type": "code",
        "outputId": "3a5e3bb5-19b0-49ef-c702-27d4d9eadc25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "with open(\"./train.enzh.src\", \"r\") as enzh_src:\n",
        "  print(\"Source: \",enzh_src.readline())\n",
        "with open(\"./train.enzh.mt\", \"r\") as enzh_mt:\n",
        "  print(\"Translation: \",enzh_mt.readline())\n",
        "with open(\"./train.enzh.scores\", \"r\") as enzh_scores:\n",
        "  print(\"Score: \",enzh_scores.readline())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source:  The last conquistador then rides on with his sword drawn.\n",
            "\n",
            "Translation:  最后的征服者骑着他的剑继续前进.\n",
            "\n",
            "Score:  -1.5284005772625449\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV-JSE2sZzoT",
        "colab_type": "text"
      },
      "source": [
        "### English Models Setup\n",
        "\n",
        "Download English models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkP0aR4rZbVA",
        "colab_type": "code",
        "outputId": "0becff7b-c0b8-4489-a4bf-b046626bc71b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "!spacy download en_core_web_md\n",
        "!spacy link en_core_web_md en300"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 1.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126236 sha256=7d3c5e3a31b8011666ca96d2dff87d112c6bec09923673a12ed8ff52973f189e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vir_3mu6/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en300\n",
            "You can now load the model via spacy.load('en300')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYByRCqQaQDH",
        "colab_type": "text"
      },
      "source": [
        "Load a GloVe English model with dim 100, and spaCy English model with dim 300.\n",
        "\n",
        "Some Chinese models only have **dim 100**, so we will need to **tokenize with spaCy, then embed with GloVe**.\n",
        "\n",
        "Other Chinese models have **dim 300**, so we can **just use spaCy**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVfblxyVaO3r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ffb1fa69-04ea-45e8-e0ba-69f6aec4a80d"
      },
      "source": [
        "import torchtext\n",
        "import spacy\n",
        "\n",
        "# Embedding for English when dim 100\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
        "\n",
        "# Tokenizer for English when dim 100, Tokenizer and Embedding when dim 300\n",
        "nlp_en = spacy.load('en300')\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:25, 2.23MB/s]                           \n",
            "100%|█████████▉| 399916/400000 [00:32<00:00, 20399.67it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tWbs9PucbS0",
        "colab_type": "text"
      },
      "source": [
        "Functions for processing English dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoKMJIr5acPj",
        "colab_type": "code",
        "outputId": "0d1a9574-a9da-4738-823f-d2168ca880c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#downloading stopwords from the nltk package\n",
        "download('stopwords') #stopwords dictionary, run once\n",
        "stop_words_en = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def preprocess_en(sentence, nlp):\n",
        "    text = sentence.lower()\n",
        "    doc = [token.lemma_ for token in  nlp.tokenizer(text)]\n",
        "    doc = [word for word in doc if word not in stop_words_en]\n",
        "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
        "    return doc\n",
        "\n",
        "def get_word_vector_en(embeddings, word):\n",
        "    try:\n",
        "      vec = embeddings.vectors[embeddings.stoi[word]]\n",
        "      return vec\n",
        "    except KeyError:\n",
        "      #print(f\"Word {word} does not exist\")\n",
        "      pass\n",
        "\n",
        "def get_sentence_vector_en(embeddings, line):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    emb = get_word_vector_en(embeddings, w)\n",
        "    #do not add if the word is out of vocabulary\n",
        "    if emb is not None:\n",
        "      vectors.append(emb)\n",
        "   \n",
        "  return torch.mean(torch.stack(vectors))\n",
        "\n",
        "def get_sentence_emb_en(line, nlp):\n",
        "  text = line.lower()\n",
        "  l = [token.lemma_ for token in nlp.tokenizer(text)]\n",
        "  l = ' '.join([word for word in l if word not in stop_words_en])\n",
        "\n",
        "  sen = nlp(l)\n",
        "  return sen.vector\n",
        "\n",
        "\n",
        "# By default we assume dim of Chinese model will be 100, and so we will need\n",
        "# to embed English model with dim 100.\n",
        "# If using Chinese model with dim 300, set dim=300.\n",
        "def get_embeddings_en(f, embeddings, nlp, dim=100):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "\n",
        "  if dim == 300:\n",
        "    for l in lines:\n",
        "      vec = get_sentence_emb_en(l, nlp)\n",
        "      if vec is not None:\n",
        "        vec = np.mean(vec)\n",
        "        sentences_vectors.append(vec)\n",
        "      else:\n",
        "        sentences_vectors.append(0)\n",
        "    return sentences_vectors\n",
        "\n",
        "  for l in lines:\n",
        "    sentence = preprocess_en(l, nlp)\n",
        "    try:\n",
        "      vec = get_sentence_vector_en(embeddings, sentence)\n",
        "      sentences_vectors.append(vec)\n",
        "    except:\n",
        "      sentences_vectors.append(0)\n",
        "\n",
        "  return sentences_vectors\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1930-s6fN7T",
        "colab_type": "text"
      },
      "source": [
        "### Chinese Models Setup\n",
        "\n",
        "Download Chinese stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O14b4JeHcNhB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "f66c24a0-b916-42a4-a4d3-1632de4e43ad"
      },
      "source": [
        "!wget -c https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-12 13:14:43--  https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘chinese_stop_words.txt’\n",
            "\n",
            "\rchinese_stop_words.     [<=>                 ]       0  --.-KB/s               \rchinese_stop_words.     [ <=>                ] 419.05K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-02-12 13:14:43 (13.1 MB/s) - ‘chinese_stop_words.txt’ saved [429109]\n",
            "\n",
            "--2020-02-12 13:14:44--  http://vectors.nlpl.eu/repository/20/35.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.225\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.225|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1458485917 (1.4G) [application/zip]\n",
            "Saving to: ‘zh_100.zip’\n",
            "\n",
            "zh_100.zip          100%[===================>]   1.36G  17.2MB/s    in 89s     \n",
            "\n",
            "2020-02-12 13:16:14 (15.6 MB/s) - ‘zh_100.zip’ saved [1458485917/1458485917]\n",
            "\n",
            "Archive:  zh_100.zip\n",
            "  inflating: ./zh_100/LIST           \n",
            "  inflating: ./zh_100/meta.json      \n",
            "  inflating: ./zh_100/model.bin      \n",
            "  inflating: ./zh_100/model.txt      \n",
            "  inflating: ./zh_100/README         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw437fJCip68",
        "colab_type": "text"
      },
      "source": [
        "Download and load Chinese model with **dim 100** (University of Oslo):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8V-nUwUikda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -O zh_100.zip http://vectors.nlpl.eu/repository/20/35.zip\n",
        "\n",
        "!unzip zh_100.zip -d ./zh_100\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "wv_from_bin_100 = KeyedVectors.load_word2vec_format(\"./zh_100/model.bin\", binary=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_QhNHjwl0rz",
        "colab_type": "text"
      },
      "source": [
        "Download and load Chinese moel with **dim 300** (Kyubyong):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p8qkZi_ivhZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "9765431e-915e-4cbd-bde1-53f61c3fabbf"
      },
      "source": [
        "!pip install gdown\n",
        "\n",
        "!gdown -O zh_300.zip https://drive.google.com/uc?id=0B0ZXk88koS2KNER5UHNDY19pbzQ\n",
        "\n",
        "!unzip zh_300.zip -d ./zh_300\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "wv_from_bin_300 = Word2Vec.load(\"./zh_300/zh.bin\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown) (4.28.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown) (2019.11.28)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=0B0ZXk88koS2KNER5UHNDY19pbzQ\n",
            "To: /content/zh_300.zip\n",
            "203MB [00:04, 42.8MB/s]\n",
            "Archive:  zh_300.zip\n",
            "  inflating: ./zh_300/zh.bin         \n",
            "  inflating: ./zh_300/zh.tsv         \n",
            "  inflating: ./zh_300/zh.bin.syn1neg.npy  \n",
            "  inflating: ./zh_300/zh.bin.syn0.npy  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo5vhydafzC5",
        "colab_type": "text"
      },
      "source": [
        "Functions for processing Chinese dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s27T7LXlf4sG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import jieba\n",
        "import gensim \n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "stop_words = [ line.rstrip() for line in open('./chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
        "\n",
        "\n",
        "def get_sentence_vector_zh(line, word_vectors):\n",
        "  vectors = []\n",
        "  for w in line:\n",
        "    try:\n",
        "      emb = word_vectors[w]\n",
        "      vectors.append(emb)\n",
        "    except:\n",
        "      pass #Do not add if the word is out of vocabulary\n",
        "  if vectors:\n",
        "    vectors = np.array(vectors)\n",
        "    return np.mean(vectors)  \n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "\n",
        "def processing_zh(sentence):\n",
        "  seg_list = jieba.lcut(sentence,cut_all=True)\n",
        "  doc = [word for word in seg_list if word not in stop_words]\n",
        "  docs = [e for e in doc if e.isalnum()]\n",
        "  return docs\n",
        "\n",
        "\n",
        "def get_embeddings_zh(f, word_vectors):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences_vectors =[]\n",
        "  for l in lines:\n",
        "    sent  = processing_zh(l)\n",
        "    vec = get_sentence_vector_zh(sent, word_vectors)\n",
        "\n",
        "    if vec is not None:\n",
        "      sentences_vectors.append(vec)\n",
        "    else:\n",
        "      print(l)\n",
        "  return sentences_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI0BAckemQOU",
        "colab_type": "text"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ6nLoxEmWN-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8ea352d6-28a0-4977-ab30-9b7f80d51b10"
      },
      "source": [
        "import spacy\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "\n",
        "zh_train_mt_100 = get_embeddings_zh(\"./train.enzh.mt\", wv_from_bin_100)\n",
        "zh_train_mt_300 = get_embeddings_zh(\"./train.enzh.mt\", wv_from_bin_300)\n",
        "zh_train_src_100 = get_embeddings_en(\"./train.enzh.src\", glove, nlp_en, dim=100)\n",
        "zh_train_src_300 = get_embeddings_en(\"./train.enzh.src\", glove, nlp_en, dim=300)\n",
        "f_train_scores = open(\"./train.enzh.scores\", 'r')\n",
        "zh_train_scores = f_train_scores.readlines()\n",
        "\n",
        "zh_val_mt_100 = get_embeddings_zh(\"./dev.enzh.mt\", wv_from_bin_100)\n",
        "zh_val_mt_300 = get_embeddings_zh(\"./dev.enzh.mt\", wv_from_bin_300)\n",
        "zh_val_src_100 = get_embeddings_en(\"./dev.enzh.src\", glove, nlp_en, dim=100)\n",
        "zh_val_src_300 = get_embeddings_en(\"./dev.enzh.src\", glove, nlp_en, dim=300)\n",
        "f_val_scores = open(\"./dev.enzh.scores\", 'r')\n",
        "zh_val_scores = f_val_scores.readlines()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXyQxiYxnSTV",
        "colab_type": "text"
      },
      "source": [
        "Check embedded correctly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h_nqGmxnRBO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "2c8d4a83-10d3-40fc-d372-90f27742e0e4"
      },
      "source": [
        "print(f\"Training mt (100): {len(zh_train_mt_100)} Training mt (300): {len(zh_train_mt_300)} Training src (100): {len(zh_train_src_100)} Training src (300): {len(zh_train_src_300)}\")\n",
        "print()\n",
        "print(f\"Validation mt (100): {len(zh_val_mt_100)} Validation mt (300): {len(zh_val_mt_300)} Validation src (100): {len(zh_val_src_100)} Validation src (300): {len(zh_val_src_300)}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training mt (100): 7000 Training mt (300): 7000 Training src (100): 7000 Training src (300): 7000\n",
            "\n",
            "Validation mt (100): 1000 Validation mt (300): 1000 Validation src (100): 1000 Validation src (300): 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg4YoR5eoZU3",
        "colab_type": "text"
      },
      "source": [
        "Setup input and predicted outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXRO34atmqiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X_train_100 = [np.array(zh_train_src_100), np.array(zh_train_mt_100)]\n",
        "X_train_zh_100 = np.array(X_train_100).transpose()\n",
        "\n",
        "X_val_100 = [np.array(zh_val_src_100),np.array(zh_val_mt_100)]\n",
        "X_val_zh_100 = np.array(X_val_100).transpose()\n",
        "\n",
        "X_train_300 = [np.array(zh_train_src_300), np.array(zh_train_mt_300)]\n",
        "X_train_zh_300 = np.array(X_train_300).transpose()\n",
        "\n",
        "X_val_300 = [np.array(zh_val_src_300),np.array(zh_val_mt_300)]\n",
        "X_val_zh_300 = np.array(X_val_300).transpose()\n",
        "\n",
        "#Scores\n",
        "train_scores = np.array(zh_train_scores).astype(float)\n",
        "y_train_zh = train_scores\n",
        "\n",
        "val_scores = np.array(zh_val_scores).astype(float)\n",
        "y_val_zh = val_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLV1j47Vo97i",
        "colab_type": "text"
      },
      "source": [
        "## Methods\n",
        "\n",
        "**TODO** e.g. SVM, random forest etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_eVxCFdo8kJ",
        "colab_type": "text"
      },
      "source": [
        "## Results\n",
        "\n",
        "(Haven't tested the function yet...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XimIq82so7Op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def writeScores(scores):\n",
        "    fn = \"predictions.txt\"\n",
        "    print(\"\")\n",
        "    with open(fn, 'w') as output_file:\n",
        "        for idx,x in enumerate(scores):\n",
        "            #out =  metrics[idx]+\":\"+str(\"{0:.2f}\".format(x))+\"\\n\"\n",
        "            #print(out)\n",
        "            output_file.write(f\"{x}\\n\")\n",
        "\n",
        "\n",
        "def downloadScores(method_name, scores):\n",
        "  writeScores(scores)\n",
        "  with ZipFile(f\"en-zh_{method_name}.zip\", \"w\") as newzip:\n",
        "    newzip.write(\"predictions.txt\")\n",
        "  \n",
        "  files.download(f\"en-zh_{method_name}.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "471QN-wLp1ER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}