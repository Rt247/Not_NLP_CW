{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_CW.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rt247/Not_NLP_CW/blob/BERT_method/NLP_CW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K43YyyuigmRt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib\n",
        "import json\n",
        "\n",
        "url_hash = \"384830ba34a2c9a1dfeb869062d8c6dc/raw/729c688d2bb4801a135a1f3824f30311a84de788\"\n",
        "\n",
        "data_set_load = {\n",
        "  \"url_dev_enzh_mt\": f'https://gist.githubusercontent.com/cheukwing/{url_hash}/dev.enzh.mt',\n",
        "  \"url_dev_enzh_scores\": f'https://gist.githubusercontent.com/cheukwing/{url_hash}/dev.enzh.scores',\n",
        "  \"url_dev_enzh_src\": f'https://gist.githubusercontent.com/cheukwing/{url_hash}/dev.enzh.src',\n",
        "  \"url_train_enzh_mt\": f'https://gist.githubusercontent.com/cheukwing/{url_hash}/train.enzh.mt',\n",
        "  \"url_train_enzh_scores\": f'https://gist.githubusercontent.com/cheukwing/{url_hash}/train.enzh.scores',\n",
        "  \"url_train_enzh_src\": f'https://gist.githubusercontent.com/cheukwing/{url_hash}/train.enzh.src',\n",
        "  \"url_test_enzh_src\": f'https://gist.githubusercontent.com/cheukwing/{url_hash}/test.enzh.src',\n",
        "  \"url_test_enzh_mt\": f'https://gist.githubusercontent.com/cheukwing/{url_hash}/test.enzh.mt',\n",
        "}\n",
        "\n",
        "data_sets = {}\n",
        "for key, value in data_set_load.items():\n",
        "  r = urllib.request.urlopen(value)\n",
        "  data = r.read().splitlines()\n",
        "  data_sets[key[4:]] = data\n",
        "  print(data_sets[key[4:]][0])\n",
        "\n",
        "print(data_sets.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCqM6ypNjctp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_bUN_tIaTs5U",
        "colab": {}
      },
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "original_texts = [text.decode() for text in data_sets['train_enzh_src']]\n",
        "translated_texts = [text.decode('utf8') for text in data_sets['train_enzh_mt']]\n",
        "\n",
        "text_pairs = list(zip(original_texts, translated_texts))\n",
        "\n",
        "\n",
        "inputs = [tokenizer.encode_plus(original, text_pair=translated, add_special_tokens = True, pad_to_max_length=True) for original, translated in text_pairs]\n",
        "\n",
        "input_ids = [d['input_ids'] for d in inputs]\n",
        "input_segments = [d['token_type_ids'] for d in inputs]\n",
        "input_attention_masks = [d['attention_mask'] for d in inputs]\n",
        "\n",
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor(input_ids)\n",
        "segments_tensor = torch.tensor(input_segments)\n",
        "attention_masks_tensor = torch.tensor(input_attention_masks)\n",
        "\n",
        "# # Load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
        "\n",
        "# # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KJCoozkTwDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Predict hidden states features for each layer\n",
        "# with torch.no_grad():\n",
        "#     last_hidden_states = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "# features = last_hidden_states[0][:][0][:].numpy()\n",
        "# print(features)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}