{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Extraction NLP_CW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WyydrvZJAnI",
        "colab_type": "text"
      },
      "source": [
        "Library imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCuY6EL4JDYY",
        "colab_type": "code",
        "outputId": "f3b05206-0d15-4bac-965f-4c555a61e5bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        }
      },
      "source": [
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!pip install stanfordnlp"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.9)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.17.5)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: stanfordnlp in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (2.21.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.17.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (4.28.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (45.1.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4g-Th0ZLkcG",
        "colab_type": "text"
      },
      "source": [
        "Download the Stanford Core NLP Chinese and English model\n",
        "\n",
        "Note: You have to manually type 'Y' and press enter to run this cell below\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLAh4TY0TyY-",
        "colab_type": "code",
        "outputId": "278f5669-de79-41bd-e96f-0de52e27fe25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "import stanfordnlp\n",
        "stanfordnlp.download('zh')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the default treebank \"zh_gsd\" for language \"zh\".\n",
            "Would you like to download the models for: zh_gsd now? (Y/n)\n",
            "y\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "\n",
            "Downloading models for: zh_gsd\n",
            "Download location: /root/stanfordnlp_resources/zh_gsd_models.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 234M/234M [00:34<00:00, 5.60MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Download complete.  Models saved to: /root/stanfordnlp_resources/zh_gsd_models.zip\n",
            "Extracting models file for: zh_gsd\n",
            "Cleaning up...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1Fyhh_2iYcf",
        "colab_type": "text"
      },
      "source": [
        "Note: You have to manually type 'Y' and press enter to run this cell below\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIrWkzcFPB7i",
        "colab_type": "code",
        "outputId": "da17138a-c042-462e-faa2-f21690114905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "import stanfordnlp\n",
        "stanfordnlp.download('en')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the default treebank \"en_ewt\" for language \"en\".\n",
            "Would you like to download the models for: en_ewt now? (Y/n)\n",
            "y\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "\n",
            "Downloading models for: en_ewt\n",
            "Download location: /root/stanfordnlp_resources/en_ewt_models.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235M/235M [01:01<00:00, 3.72MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Download complete.  Models saved to: /root/stanfordnlp_resources/en_ewt_models.zip\n",
            "Extracting models file for: en_ewt\n",
            "Cleaning up...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6BshlcaZgmT",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Download datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzekR1EZY_5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os.path import exists\n",
        "\n",
        "if not exists('enzh_data.zip'):\n",
        "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
        "    !unzip enzh_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UskRgN-6ZoKo",
        "colab_type": "text"
      },
      "source": [
        "Check data downloaded successfully:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQUZyQrXY_5a",
        "colab_type": "code",
        "outputId": "a7c89f37-517e-4e4e-fa54-57b1e6e18264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "with open(\"./train.enzh.src\", \"r\") as enzh_src:\n",
        "  print(\"Source: \",enzh_src.readline())\n",
        "with open(\"./train.enzh.mt\", \"r\") as enzh_mt:\n",
        "  print(\"Translation: \",enzh_mt.readline())\n",
        "with open(\"./train.enzh.scores\", \"r\") as enzh_scores:\n",
        "  print(\"Score: \",enzh_scores.readline())"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source:  The last conquistador then rides on with his sword drawn.\n",
            "\n",
            "Translation:  最后的征服者骑着他的剑继续前进.\n",
            "\n",
            "Score:  -1.5284005772625449\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV-JSE2sZzoT",
        "colab_type": "text"
      },
      "source": [
        "### English Models Setup\n",
        "\n",
        "Download English models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkP0aR4rZbVA",
        "colab_type": "code",
        "outputId": "98d91de9-4cc8-4e03-90bf-c0c35876b19d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "!spacy download en_core_web_md\n",
        "!spacy link en_core_web_md en300"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_md==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz#egg=en_core_web_md==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "\n",
            "\u001b[38;5;1m✘ Link 'en300' already exists\u001b[0m\n",
            "To overwrite an existing link, use the --force flag\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVfblxyVaO3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "import spacy\n",
        "\n",
        "nlp_en = spacy.load('en300')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tWbs9PucbS0",
        "colab_type": "text"
      },
      "source": [
        "Functions for processing English dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoKMJIr5acPj",
        "colab_type": "code",
        "outputId": "a7ccc1b7-efc0-4883-cd76-3cba18afc0fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#downloading stopwords from the nltk package\n",
        "download('stopwords') #stopwords dictionary, run once\n",
        "stop_words_en = set(stopwords.words('english'))\n",
        "\n",
        "def get_sentence_array(f):\n",
        "    file = open(f) \n",
        "    lines = file.readlines() \n",
        "    return lines \n",
        "\n",
        "def preprocess_en(sentence, nlp):\n",
        "    text = sentence.lower()\n",
        "    doc = [token.lemma_ for token in  nlp.tokenizer(text)]\n",
        "    doc = [word for word in doc if word not in stop_words_en]\n",
        "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
        "    return doc\n",
        "\n",
        "def get_english_sentences(f, nlp, preprocess=True):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences = []\n",
        "\n",
        "  for l in lines:\n",
        "    if preprocess: \n",
        "      sentence = preprocess_en(l, nlp)\n",
        "    else: \n",
        "      sentence = [token.lemma_ for token in  nlp.tokenizer(l)]\n",
        "      sentence = list(filter(None, sentence))\n",
        "    sentences.append(sentence)\n",
        "\n",
        "  return sentences\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1930-s6fN7T",
        "colab_type": "text"
      },
      "source": [
        "### Chinese Models Setup\n",
        "\n",
        "Download Chinese stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O14b4JeHcNhB",
        "colab_type": "code",
        "outputId": "eb6d4647-3d5c-4092-9898-b2ac5c4b2324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "!wget -c https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-27 21:14:21--  https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
            "Resolving github.com (github.com)... 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘chinese_stop_words.txt’\n",
            "\n",
            "chinese_stop_words.     [   <=>              ] 419.55K   480KB/s    in 0.9s    \n",
            "\n",
            "2020-02-27 21:14:23 (480 KB/s) - ‘chinese_stop_words.txt’ saved [429623]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo5vhydafzC5",
        "colab_type": "text"
      },
      "source": [
        "Functions for processing Chinese dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s27T7LXlf4sG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import jieba\n",
        "import gensim \n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "stop_words = [ line.rstrip() for line in open('./chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
        "\n",
        "def processing_zh(sentence):\n",
        "  seg_list = jieba.lcut(sentence,cut_all=True)\n",
        "  doc = [word for word in seg_list if word not in stop_words]\n",
        "  docs = [e for e in doc if e.isalnum()]\n",
        "  return docs\n",
        "\n",
        "\n",
        "def get_chinese_sentences(f, preprocess=True):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences =[]\n",
        "  for l in lines:\n",
        "    if preprocess: \n",
        "      sent  = processing_zh(l)\n",
        "    else: \n",
        "      sent = jieba.lcut(l,cut_all=True)\n",
        "      sent = list(filter(None, sent))\n",
        "    sentences.append(sent)\n",
        "  return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI0BAckemQOU",
        "colab_type": "text"
      },
      "source": [
        "## Feature Extraction: Complexity / Fluency of Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHdd5mVt_57c",
        "colab_type": "text"
      },
      "source": [
        "Extract number of tokens in source segment and target segment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHNM5UqeDq6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_num_token(data_set_token):\n",
        "  num_tokens = []\n",
        "  for sentence in data_set_token: \n",
        "    num_tokens.append(len(sentence))\n",
        "  return num_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuWG6_yeGgWp",
        "colab_type": "text"
      },
      "source": [
        "N-gram language model probability of source segment using the source side of the parallel corpus used to train the MT system as LM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oil351YZGqIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "def get_lm_prob_src(dataset):\n",
        "\n",
        "  num_src_prob = []\n",
        "  model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "  # Count frequency of co-occurance and store in dictionary \n",
        "  for sentence in dataset:\n",
        "      for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "          model[(w1, w2)][w3] += 1\n",
        "\n",
        "  # Transform the counts to probabilities over total count \n",
        "  for w1_w2 in model:\n",
        "      total_count = float(sum(model[w1_w2].values()))\n",
        "      for w3 in model[w1_w2]:\n",
        "          model[w1_w2][w3] /= total_count\n",
        "\n",
        "  # Calculate the trigram estimate of sentence probabilities  \n",
        "  # p(Sentence) = P(W1, ... Wn)\n",
        "  for sentence in dataset:\n",
        "    sentence_prob = 1\n",
        "    for i in range(len(sentence) - 2):\n",
        "      sentence_prob = sentence_prob * dict(model[sentence[i], sentence[i + 1]])[sentence[i + 2]]\n",
        "    num_src_prob.append(sentence_prob)\n",
        "\n",
        "  return num_src_prob\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUrovyNYX7NT",
        "colab_type": "text"
      },
      "source": [
        "Load a large corpus of the target language to build the LM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOi_zpyWX7V4",
        "colab_type": "code",
        "outputId": "1ae0f5e7-6bdc-4682-9d73-a83e48dddfe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "source": [
        "# Load the Chinese Corpus\n",
        "\n",
        "!wget http://pcai056.informatik.uni-leipzig.de/downloads/corpora/zho-simp-tw_web_2014_10K.tar.gz\n",
        "!tar -zxvf zho-simp-tw_web_2014_10K.tar.gz\n",
        "\n",
        "zho_sentence_path = \"./zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-sentences.txt\""
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-27 21:14:25--  http://pcai056.informatik.uni-leipzig.de/downloads/corpora/zho-simp-tw_web_2014_10K.tar.gz\n",
            "Resolving pcai056.informatik.uni-leipzig.de (pcai056.informatik.uni-leipzig.de)... 139.18.2.216\n",
            "Connecting to pcai056.informatik.uni-leipzig.de (pcai056.informatik.uni-leipzig.de)|139.18.2.216|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3915364 (3.7M) [application/x-gzip]\n",
            "Saving to: ‘zho-simp-tw_web_2014_10K.tar.gz.2’\n",
            "\n",
            "zho-simp-tw_web_201 100%[===================>]   3.73M  1.70MB/s    in 2.2s    \n",
            "\n",
            "2020-02-27 21:14:28 (1.70 MB/s) - ‘zho-simp-tw_web_2014_10K.tar.gz.2’ saved [3915364/3915364]\n",
            "\n",
            "zho-simp-tw_web_2014_10K/\n",
            "zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-sources.txt\n",
            "zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-words.txt\n",
            "zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-inv_w.txt\n",
            "zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-sentences.txt\n",
            "zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-import.sql\n",
            "zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-co_n.txt\n",
            "zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-inv_so.txt\n",
            "zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-co_s.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ3WXiS4gtoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process_zho_chinese_sentences(f):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  lines = lines[1:]\n",
        "  sentences = []\n",
        "  chop_counter = 1\n",
        "  for i, l in enumerate(lines):\n",
        "    if i >= pow(10, chop_counter): \n",
        "      chop_counter += 1\n",
        "    processed_sentence = processing_zh(l[(chop_counter + 1):])\n",
        "    sentences.append(processed_sentence)\n",
        "  return sentences\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ4-Bq-PoKOa",
        "colab_type": "text"
      },
      "source": [
        "Build N Gram Language Model probability of target segment with larger dataset and input mt corpus "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j-_3c1SoIh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "def get_lm_prob_target(dataset):\n",
        "\n",
        "  processed_sentences = pre_process_zho_chinese_sentences(zho_sentence_path)\n",
        "\n",
        "  processed_sentences.extend(dataset)\n",
        "\n",
        "  num_mt_prob = []\n",
        "  model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "  # Count frequency of co-occurance and store in dictionary\n",
        "  for sentence in processed_sentences:\n",
        "      for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "          model[(w1, w2)][w3] += 1\n",
        "\n",
        "  # Transform the counts to probabilities over total count \n",
        "  for w1_w2 in model:\n",
        "      total_count = float(sum(model[w1_w2].values()))\n",
        "      for w3 in model[w1_w2]:\n",
        "          model[w1_w2][w3] /= total_count\n",
        "\n",
        "  # Calculate the trigram estimate of sentence probabilities  \n",
        "  # p(Sentence) = P(W1, ... Wn)\n",
        "  for sentence in dataset:\n",
        "    for i in range(len(sentence) - 2):\n",
        "      sentence_prob = 1\n",
        "      sentence_prob = dict(model[sentence[i], sentence[i + 1]])[sentence[i + 2]]\n",
        "    num_mt_prob.append(sentence_prob)\n",
        "\n",
        "  return num_mt_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTCEB_wgql5e",
        "colab_type": "text"
      },
      "source": [
        "# Feature Extraction: Adequacy of Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uFoIGlWrQ1X",
        "colab_type": "text"
      },
      "source": [
        "Ratio of number of tokens in source and target segments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa3hfkzlrWLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ratio_num_token(num_src_tokens, num_mt_tokens):\n",
        "  ratio_token_src_mt = list(map(lambda src, mt: src / mt, num_src_tokens, num_mt_tokens)) \n",
        "  return ratio_token_src_mt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3ZxBUYzvzbr",
        "colab_type": "text"
      },
      "source": [
        "Ratio of brackets and punctuation symbols in source and target segments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guhmtlib7dM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections as ct\n",
        "\n",
        "def get_punc_ratio_sent(src_sentence, mt_sentence):\n",
        "\n",
        "  src_dict = {c:val for c, val in ct.Counter(src_sentence).items() if c in string.punctuation}\n",
        "  mt_dict = {c:val for c, val in ct.Counter(mt_sentence).items() if c in string.punctuation}\n",
        "\n",
        "  if sum(mt_dict.values())!= 0: \n",
        "    return sum(src_dict.values()) / sum(mt_dict.values()) \n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def get_ratio_punc(pro_dataset_src, pro_dataset_mt):\n",
        "  ratio_punc_src_mt = list(map(get_punc_ratio_sent, pro_dataset_src, pro_dataset_mt))\n",
        "  return ratio_punc_src_mt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxdaSbeR-6Pr",
        "colab_type": "text"
      },
      "source": [
        "Ratio of open class words, closed class words and other in the source & target segments defined in https://universaldependencies.org/u/pos/\n",
        "\n",
        "Ratio of percentage of nouns / verbs etc... in the source and target segments\n",
        "\n",
        "Difference between the depth of the syntactic trees of the source and target segments "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pfe-9ed-6YP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import stanfordnlp\n",
        "import warnings\n",
        "from itertools import chain \n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "open_class_words = [\"ADJ\", \"ADV\", \"INTJ\", \"NOUN\", \"PROPN\", \"VERB\"]\n",
        "closed_class_words = [\"ADP\", \"AUX\", \"CCONJ\", \"DET\", \"NUM\", \"PART\", \"PRON\", \"SCONJ\"]\n",
        "other_words = [\"PUNCT\", \"SYM\", \"X\"]\n",
        "\n",
        "all_tag_list = list(chain(open_class_words, closed_class_words, other_words))\n",
        "\n",
        "def get_coarse_tag_count(sentence): \n",
        "\n",
        "  sent_open_class_words = 0\n",
        "  sent_closed_class_words = 0\n",
        "  sent_other_words = 0\n",
        "\n",
        "  for word in sentence:\n",
        "    if word.upos in open_class_words:\n",
        "      sent_open_class_words += 1\n",
        "    elif word.upos in closed_class_words:\n",
        "      sent_closed_class_words += 1\n",
        "    else: \n",
        "      sent_other_words += 1\n",
        "\n",
        "  return sent_open_class_words, sent_closed_class_words, sent_other_words\n",
        "\n",
        "def get_fine_tag_count(sentence): \n",
        "\n",
        "  tag_dict = {\n",
        "    \"ADJ\": 0, \n",
        "    \"ADV\": 0,\n",
        "    \"INTJ\": 0, \n",
        "    \"NOUN\": 0, \n",
        "    \"PROPN\": 0, \n",
        "    \"VERB\": 0,\n",
        "    \"ADP\": 0, \n",
        "    \"AUX\": 0,\n",
        "    \"CCONJ\": 0, \n",
        "    \"DET\": 0, \n",
        "    \"NUM\": 0, \n",
        "    \"PART\": 0, \n",
        "    \"PRON\": 0, \n",
        "    \"SCONJ\": 0,\n",
        "    \"PUNCT\": 0, \n",
        "    \"SYM\": 0, \n",
        "    \"X\": 0\n",
        "  }\n",
        "\n",
        "  tag_list = [word.upos for word in sentence] \n",
        "\n",
        "  sentence_dict = dict(Counter(tag_list))\n",
        "  \n",
        "  tag_dict.update(sentence_dict) \n",
        "  return tag_dict\n",
        "\n",
        "def gen_tree(nodes, root):\n",
        "  tree = defaultdict(dict)\n",
        "\n",
        "  for child, parent in nodes: \n",
        "    tree[parent][child] = tree[child]\n",
        "\n",
        "  try:\n",
        "    children, parents = zip(*nodes)\n",
        "    root = set(parents).difference(children).pop()\n",
        "  except:\n",
        "    print(nodes)\n",
        "    raise NameError('HiThere')\n",
        "\n",
        "  return {root: tree[root]}\n",
        "\n",
        "def dict_depth(tree_dict): \n",
        "    if isinstance(tree_dict, dict):    \n",
        "        return 1 + (max(map(dict_depth, tree_dict.values())) \n",
        "                                    if tree_dict else 0) \n",
        "    return 0\n",
        "\n",
        "def get_dependency_count(sentence): \n",
        "\n",
        "  root_relation = \"root\"\n",
        "  nodes = []\n",
        "  for word in sentence: \n",
        "    if word.dependency_relation == 'root':\n",
        "      root_relation = word.text\n",
        "    else: \n",
        "      nodes.append((word.text, word.governor))\n",
        "  \n",
        "  if not nodes: \n",
        "    return {}\n",
        "\n",
        "  tree_dict = gen_tree(nodes, root_relation)\n",
        "  return tree_dict\n",
        "\n",
        "\n",
        "def get_tag_dep_ratio(src_sentence, mt_sentence):\n",
        "\n",
        "  # Process sentences\n",
        "  src_doc = nlp_stan_en(src_sentence)\n",
        "  mt_doc = nlp_stan_chinese(mt_sentence)\n",
        "\n",
        "  # Get dependency counts \n",
        "  src_tree_dict = get_dependency_count(src_doc.sentences[0].words)\n",
        "  mt_tree_dict = get_dependency_count(mt_doc.sentences[0].words) \n",
        "\n",
        "  depth_diff = abs(dict_depth(src_tree_dict) - dict_depth(mt_tree_dict))\n",
        "\n",
        "  # Get course tag ratios\n",
        "  src_oc_w, src_cc_w, src_o_w = get_coarse_tag_count(src_doc.sentences[0].words)\n",
        "  mt_oc_w, mt_cc_w, mt_o_w = get_coarse_tag_count(mt_doc.sentences[0].words)\n",
        "\n",
        "  oc_ratio = src_oc_w / mt_oc_w if mt_oc_w != 0 else 0 \n",
        "  cc_ratio = src_cc_w / mt_cc_w if mt_cc_w != 0 else 0 \n",
        "  o_ratio = src_o_w / mt_o_w if mt_o_w != 0 else 0 \n",
        "\n",
        "  # Get fine tag ratios\n",
        "  src_tag_dict = get_fine_tag_count(src_doc.sentences[0].words)\n",
        "  mt_tag_dict = get_fine_tag_count(mt_doc.sentences[0].words)\n",
        "\n",
        "  ratio_tags_fine_grain_sent = []\n",
        "  for tag in all_tag_list:\n",
        "    ratio = src_tag_dict[tag] / mt_tag_dict[tag] if mt_tag_dict[tag] != 0 else 0 \n",
        "    ratio_tags_fine_grain_sent.append(ratio)\n",
        "\n",
        "  ratio_dict = {\n",
        "      \"oc_ratio\": oc_ratio, \n",
        "      \"cc_ratio\": cc_ratio,\n",
        "      \"o_ratio\": o_ratio,\n",
        "      \"ratio_tags_fine_grain_sent\": ratio_tags_fine_grain_sent,\n",
        "      \"depth_diff\": depth_diff\n",
        "  }\n",
        "\n",
        "  return ratio_dict\n",
        "\n",
        "def get_ratio_tags_depend(raw_dataset_src, raw_dataset_mt):\n",
        "\n",
        "  ratio_list_of_dict = list(map(get_tag_dep_ratio, raw_dataset_src, raw_dataset_mt))\n",
        "\n",
        "  ratio_oc_tags = [] \n",
        "  ratio_cc_tags = []\n",
        "  ratio_o_tags = []\n",
        "  ratio_tags_fine = []\n",
        "  depth_diff = []\n",
        "\n",
        "  for ratio_dict in ratio_list_of_dict:\n",
        "    ratio_oc_tags.append(ratio_dict['oc_ratio'])\n",
        "    ratio_cc_tags.append(ratio_dict['cc_ratio'])\n",
        "    ratio_o_tags.append(ratio_dict['o_ratio'])\n",
        "    ratio_tags_fine.append(ratio_dict['ratio_tags_fine_grain_sent'])\n",
        "    depth_diff.append(ratio_dict['depth_diff'])\n",
        "\n",
        "  return ratio_oc_tags, ratio_cc_tags, ratio_o_tags, ratio_tags_fine, depth_diff\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl8o0sxUXku_",
        "colab_type": "text"
      },
      "source": [
        "# Generate the feature vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXTJWa5YpJj7",
        "colab_type": "text"
      },
      "source": [
        "Load the data sets to generate the feature vectors "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ6nLoxEmWN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "import torchtext\n",
        "import numpy as np\n",
        "from torchtext import data\n",
        "\n",
        "# Train \n",
        "zh_train_mt = get_chinese_sentences(\"./train.enzh.mt\", True)\n",
        "zh_train_src = get_english_sentences(\"./train.enzh.src\", nlp_en, True)\n",
        "\n",
        "f_train_scores = open(\"./train.enzh.scores\", 'r')\n",
        "zh_train_scores = f_train_scores.readlines()\n",
        "\n",
        "pro_zh_train_mt = get_chinese_sentences(\"./train.enzh.mt\", False)\n",
        "pro_zh_train_src = get_english_sentences(\"./train.enzh.src\", nlp_en, False)\n",
        "\n",
        "raw_zh_train_mt = get_sentence_array(\"./train.enzh.mt\")\n",
        "raw_zh_train_src = get_sentence_array(\"./train.enzh.src\")\n",
        "\n",
        "# Validation\n",
        "zh_val_mt = get_chinese_sentences(\"./dev.enzh.mt\")\n",
        "zh_val_src = get_english_sentences(\"./dev.enzh.src\", nlp_en)\n",
        "\n",
        "f_val_scores = open(\"./dev.enzh.scores\", 'r')\n",
        "zh_val_scores = f_val_scores.readlines()\n",
        "\n",
        "pro_zh_val_mt = get_chinese_sentences(\"./dev.enzh.mt\", False)\n",
        "pro_zh_val_src = get_english_sentences(\"./dev.enzh.src\", nlp_en, False)\n",
        "\n",
        "raw_zh_val_mt = get_sentence_array(\"./dev.enzh.mt\")\n",
        "raw_zh_val_src = get_sentence_array(\"./dev.enzh.src\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuc9yuqnYEMO",
        "colab_type": "text"
      },
      "source": [
        "Generate the complexity feature set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqnLjljqYDcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train \n",
        "num_token_src_train = get_num_token(zh_train_src)\n",
        "# Normalise \n",
        "num_token_src_train = (num_token_src_train - np.min(num_token_src_train)) / (np.max(num_token_src_train) - np.min(num_token_src_train))\n",
        "lm_prob_src_train = get_lm_prob_src(zh_train_src)\n",
        "\n",
        "# Validation\n",
        "num_token_src_val = get_num_token(zh_val_src)\n",
        "lm_prob_src_val = get_lm_prob_src(zh_val_src)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G24SmhLb42QL",
        "colab_type": "text"
      },
      "source": [
        "Generate the fluency feature set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvcCMbnK5J7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train \n",
        "num_token_mt_train = get_num_token(zh_train_mt)\n",
        "# Normalise \n",
        "num_token_mt_train = (num_token_mt_train - np.min(num_token_mt_train)) / (np.max(num_token_mt_train) - np.min(num_token_mt_train))\n",
        "lm_prob_mt_train = get_lm_prob_target(zh_train_mt)\n",
        "\n",
        "# Validation\n",
        "num_token_mt_val = get_num_token(zh_val_mt)\n",
        "lm_prob_mt_val = get_lm_prob_target(zh_val_mt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLgENJczZrjW",
        "colab_type": "text"
      },
      "source": [
        "Generate the adaquecy feature set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iMjxIDbit-T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3fc66b79-d06d-4911-b4a3-fe1a28e7e03b"
      },
      "source": [
        "# Train \n",
        "ratio_token_src_mt_train = get_ratio_num_token(num_token_src_train, num_token_mt_train)\n",
        "ratio_punc_src_mt_train = get_ratio_punc(pro_zh_train_src, pro_zh_train_mt)\n",
        "\n",
        "# Validation\n",
        "ratio_token_src_mt_val = get_ratio_num_token(num_token_src_val, num_token_mt_val)\n",
        "ratio_punc_src_mt_val = get_ratio_punc(pro_zh_val_src, pro_zh_val_mt)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNpRYNRBDG93",
        "colab_type": "text"
      },
      "source": [
        "Note: The cell below may take 10 mins to run "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uFyAsrsZrvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train \n",
        "ratio_oc_tags_train, ratio_cc_tags_train, ratio_o_tags_train, \\\n",
        " ratio_tags_fine_train, depth_diff_train =  \\\n",
        " get_ratio_tags_depend(raw_zh_train_src, raw_zh_train_mt)\n",
        "\n",
        "# Validation \n",
        "ratio_oc_tags_val, ratio_cc_tags_val, ratio_pao_tags_val, \\\n",
        " ratio_tags_fine_val, depth_diff_val =  \\\n",
        " get_ratio_tags_depend(raw_zh_val_src, raw_zh_val_mt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va1V_LnY4s27",
        "colab_type": "text"
      },
      "source": [
        "Combine the features to create an single feature vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIGo4BqU4tIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_feature_vectors(num_token_src, lm_prob_src, \n",
        "                        num_token_mt, lm_prob_mt, \n",
        "                        ratio_token_src_mt, ratio_punc_src_mt, \n",
        "                        ratio_oc_tags, ratio_cc_tags, ratio_o_tags, \n",
        "                        ratio_tags_fine, depth_diff):\n",
        "  feature_vector_list = []\n",
        "  for i in range(0, len(num_token_src)):\n",
        "    feature_vector = [\n",
        "      num_token_src[i],\n",
        "      lm_prob_src[i],\n",
        "      num_token_mt[i],\n",
        "      lm_prob_mt[i],\n",
        "      ratio_token_src_mt[i],\n",
        "      ratio_punc_src_mt[i],\n",
        "      ratio_oc_tags[i],\n",
        "      ratio_cc_tags[i],\n",
        "      ratio_o_tags[i],    \n",
        "      depth_diff[i]          \n",
        "    ]\n",
        "    feature_vector.extend(ratio_tags_fine[i])\n",
        "    feature_vector_list.append(feature_vector)\n",
        "  return feature_vector_list\n",
        "  \n",
        "feature_vector_train = get_feature_vectors(num_token_src_train, lm_prob_src_train, \n",
        "                        num_token_mt_train, lm_prob_mt_train, \n",
        "                        ratio_token_src_mt_train, ratio_punc_src_mt_train,  \n",
        "                        ratio_oc_tags_train, ratio_cc_tags_train, ratio_o_tags_train, \n",
        "                        ratio_tags_fine_train, depth_diff_train)\n",
        "                    \n",
        "feature_vector_val = get_feature_vectors(num_token_src_val, lm_prob_src_val, \n",
        "                        num_token_mt_val, lm_prob_mt_val, \n",
        "                        ratio_token_src_mt_val, ratio_punc_src_mt_val,  \n",
        "                        ratio_oc_tags_val, ratio_cc_tags_val, ratio_o_tags_val, \n",
        "                        ratio_tags_fine_val, depth_diff_val)\n",
        "\n",
        "# print(num_token_src_train[0])\n",
        "# print(lm_prob_src_train[0])\n",
        "\n",
        "# print(num_token_mt_train[0])\n",
        "# print(lm_prob_mt_train[0])\n",
        "\n",
        "# print(ratio_token_src_mt_train[0])\n",
        "# print(ratio_punc_src_mt_train[0])\n",
        "\n",
        "# print(ratio_oc_tags_train[0])\n",
        "# print(ratio_cc_tags_train[0])\n",
        "# print(ratio_o_tags_train[0])\n",
        "\n",
        "# print(ratio_tags_fine_train[0])\n",
        "# print(depth_diff_train[0])\n",
        "\n",
        "# print(feature_vector_train[0])\n",
        "# print(feature_vector_val[0])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9hLFzLJHDrZ",
        "colab_type": "text"
      },
      "source": [
        "Create the training and label sets for train and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUGFP6_6HKsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_zh = np.array(feature_vector_train).transpose()\n",
        "y_train_zh = np.array(zh_train_scores).astype(float)\n",
        "\n",
        "X_val_zh = np.array(feature_vector_val).transpose()\n",
        "y_val_zh = np.array(zh_val_scores).astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45kbCNhMICez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math \n",
        "\n",
        "#print(np.where(np.isnan(X_train_zh)))\n",
        "for count, val in enumerate(X_train_zh):\n",
        "  if not val.any(): \n",
        "    for values in val: \n",
        "      if not val: \n",
        "        print(values)\n",
        "    X_train_zh[count] = X_train_zh[0]\n",
        "\n",
        "for count, val in enumerate(y_train_zh):\n",
        "  if not val.any(): \n",
        "    print(count)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsuPvgMCjT_u",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Regression Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzCLyRD6HyBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SELo0xgH0OO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "4bcd1ddd-8b39-4231-a260-00a8376c8c84"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "from scipy.stats.stats import pearsonr\n",
        "\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train_zh, y_train_zh)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val_zh)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val_zh)} Pearson {pearson[0]}')\n",
        "    print()"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-169-b28163f207f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'poly'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclf_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mclf_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_zh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_zh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_zh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    146\u001b[0m         X, y = check_X_y(X, y, dtype=np.float64,\n\u001b[1;32m    147\u001b[0m                          \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                          accept_large_sparse=False)\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg4YoR5eoZU3",
        "colab_type": "text"
      },
      "source": [
        "Setup input and predicted outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXRO34atmqiO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# X_train_100 = [np.array(zh_train_src_100), np.array(zh_train_mt_100)]\n",
        "# X_train_zh_100 = np.array(X_train_100).transpose()\n",
        "\n",
        "# X_val_100 = [np.array(zh_val_src_100),np.array(zh_val_mt_100)]\n",
        "# X_val_zh_100 = np.array(X_val_100).transpose()\n",
        "\n",
        "# X_train_300 = [np.array(zh_train_src_300), np.array(zh_train_mt_300)]\n",
        "# X_train_zh_300 = np.array(X_train_300).transpose()\n",
        "\n",
        "# X_val_300 = [np.array(zh_val_src_300),np.array(zh_val_mt_300)]\n",
        "# X_val_zh_300 = np.array(X_val_300).transpose()\n",
        "\n",
        "# #Scores\n",
        "# train_scores = np.array(zh_train_scores).astype(float)\n",
        "# y_train_zh = train_scores\n",
        "\n",
        "# val_scores = np.array(zh_val_scores).astype(float)\n",
        "# y_val_zh = val_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLV1j47Vo97i",
        "colab_type": "text"
      },
      "source": [
        "## Methods\n",
        "\n",
        "**TODO** e.g. SVM, random forest etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XimIq82so7Op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def writeScores(scores):\n",
        "    fn = \"predictions.txt\"\n",
        "    print(\"\")\n",
        "    with open(fn, 'w') as output_file:\n",
        "        for idx,x in enumerate(scores):\n",
        "            #out =  metrics[idx]+\":\"+str(\"{0:.2f}\".format(x))+\"\\n\"\n",
        "            #print(out)\n",
        "            output_file.write(f\"{x}\\n\")\n",
        "\n",
        "\n",
        "def downloadScores(method_name, scores):\n",
        "  writeScores(scores)\n",
        "  with ZipFile(f\"en-zh_{method_name}.zip\", \"w\") as newzip:\n",
        "    newzip.write(\"predictions.txt\")\n",
        "  \n",
        "  files.download(f\"en-zh_{method_name}.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}