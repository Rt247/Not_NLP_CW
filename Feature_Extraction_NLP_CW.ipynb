{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Extraction NLP_CW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WyydrvZJAnI",
        "colab_type": "text"
      },
      "source": [
        "Library imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCuY6EL4JDYY",
        "colab_type": "code",
        "outputId": "412349b6-ea9a-4fa7-daf6-d2398dcb2721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        }
      },
      "source": [
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!pip install stanfordnlp"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.17.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy) (4.28.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: stanfordnlp in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.17.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (3.10.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from stanfordnlp) (1.4.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanfordnlp) (2.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (45.1.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanfordnlp) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4g-Th0ZLkcG",
        "colab_type": "text"
      },
      "source": [
        "Download the Stanford Core NLP Chinese and English model\n",
        "\n",
        "Note: You have to manually type 'Y' and press enter to run this cell below\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLAh4TY0TyY-",
        "colab_type": "code",
        "outputId": "a470fd64-69a6-4aca-9f05-068145dbcd3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "import stanfordnlp\n",
        "stanfordnlp.download('zh')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the default treebank \"zh_gsd\" for language \"zh\".\n",
            "Would you like to download the models for: zh_gsd now? (Y/n)\n",
            "y\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "\n",
            "Downloading models for: zh_gsd\n",
            "Download location: /root/stanfordnlp_resources/zh_gsd_models.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 234M/234M [01:18<00:00, 3.33MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Download complete.  Models saved to: /root/stanfordnlp_resources/zh_gsd_models.zip\n",
            "Extracting models file for: zh_gsd\n",
            "Cleaning up...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1Fyhh_2iYcf",
        "colab_type": "text"
      },
      "source": [
        "Note: You have to manually type 'Y' and press enter to run this cell below\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIrWkzcFPB7i",
        "colab_type": "code",
        "outputId": "948759c7-9ec9-4f30-bb96-fff8e5afd3a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "import stanfordnlp\n",
        "stanfordnlp.download('en')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using the default treebank \"en_ewt\" for language \"en\".\n",
            "Would you like to download the models for: en_ewt now? (Y/n)\n",
            "y\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "\n",
            "Downloading models for: en_ewt\n",
            "Download location: /root/stanfordnlp_resources/en_ewt_models.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235M/235M [01:51<00:00, 2.17MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Download complete.  Models saved to: /root/stanfordnlp_resources/en_ewt_models.zip\n",
            "Extracting models file for: en_ewt\n",
            "Cleaning up...Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6BshlcaZgmT",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Download datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzekR1EZY_5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "outputId": "002a9292-cdc1-45b8-ec5c-1372ad5e127e"
      },
      "source": [
        "from os.path import exists\n",
        "\n",
        "if not exists('enzh_data.zip'):\n",
        "    !wget -O enzh_data.zip https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
        "    !unzip enzh_data.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-28 09:24:51--  https://competitions.codalab.org/my/datasets/download/03e23bd7-8084-4542-997b-6a1ca6dd8a5f\n",
            "Resolving competitions.codalab.org (competitions.codalab.org)... 129.175.22.230\n",
            "Connecting to competitions.codalab.org (competitions.codalab.org)|129.175.22.230|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=81363c2853ab8907f76ade3b79671c7714ef6faf1a90be10fa1bc55e459f1cd6&X-Amz-Date=20200228T092452Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200228%2Fnewcodalab%2Fs3%2Faws4_request [following]\n",
            "--2020-02-28 09:24:52--  https://newcodalab.lri.fr/prod-private/dataset_data_file/None/630ec/en-zh.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=86400&X-Amz-SignedHeaders=host&X-Amz-Signature=81363c2853ab8907f76ade3b79671c7714ef6faf1a90be10fa1bc55e459f1cd6&X-Amz-Date=20200228T092452Z&X-Amz-Credential=AZIAIOSAODNN7EX123LE%2F20200228%2Fnewcodalab%2Fs3%2Faws4_request\n",
            "Resolving newcodalab.lri.fr (newcodalab.lri.fr)... 129.175.15.11\n",
            "Connecting to newcodalab.lri.fr (newcodalab.lri.fr)|129.175.15.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 870893 (850K) [application/zip]\n",
            "Saving to: ‘enzh_data.zip’\n",
            "\n",
            "enzh_data.zip       100%[===================>] 850.48K   552KB/s    in 1.5s    \n",
            "\n",
            "2020-02-28 09:24:55 (552 KB/s) - ‘enzh_data.zip’ saved [870893/870893]\n",
            "\n",
            "Archive:  enzh_data.zip\n",
            "  inflating: dev.enzh.mt             \n",
            "  inflating: dev.enzh.scores         \n",
            "  inflating: dev.enzh.src            \n",
            "  inflating: test.enzh.mt            \n",
            "  inflating: test.enzh.src           \n",
            "  inflating: train.enzh.mt           \n",
            "  inflating: train.enzh.src          \n",
            "  inflating: train.enzh.scores       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UskRgN-6ZoKo",
        "colab_type": "text"
      },
      "source": [
        "Check data downloaded successfully:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQUZyQrXY_5a",
        "colab_type": "code",
        "outputId": "cab319db-3eec-4ea8-95d2-a984ccb314a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "with open(\"./train.enzh.src\", \"r\") as enzh_src:\n",
        "  print(\"Source: \",enzh_src.readline())\n",
        "with open(\"./train.enzh.mt\", \"r\") as enzh_mt:\n",
        "  print(\"Translation: \",enzh_mt.readline())\n",
        "with open(\"./train.enzh.scores\", \"r\") as enzh_scores:\n",
        "  print(\"Score: \",enzh_scores.readline())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source:  The last conquistador then rides on with his sword drawn.\n",
            "\n",
            "Translation:  最后的征服者骑着他的剑继续前进.\n",
            "\n",
            "Score:  -1.5284005772625449\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV-JSE2sZzoT",
        "colab_type": "text"
      },
      "source": [
        "### English Models Setup\n",
        "\n",
        "Download English models:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkP0aR4rZbVA",
        "colab_type": "code",
        "outputId": "750c683c-8c8d-4cbd-8523-1483ffbdc9b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "!spacy download en_core_web_md\n",
        "!spacy link en_core_web_md en300"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
            "\u001b[K     |████████████████████████████████| 95.4MB 712kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126236 sha256=f8d1a247f9f92eca78fbed5d1b754be129b7e95897138d3813be33f531e5f486\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_apmtf5a/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_md -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en300\n",
            "You can now load the model via spacy.load('en300')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVfblxyVaO3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "import spacy\n",
        "\n",
        "nlp_en = spacy.load('en300')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tWbs9PucbS0",
        "colab_type": "text"
      },
      "source": [
        "Functions for processing English dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoKMJIr5acPj",
        "colab_type": "code",
        "outputId": "d2e93e3f-69bb-49a5-9340-89dc9b0e88b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#downloading stopwords from the nltk package\n",
        "download('stopwords') #stopwords dictionary, run once\n",
        "stop_words_en = set(stopwords.words('english'))\n",
        "\n",
        "def get_sentence_array(f):\n",
        "    file = open(f) \n",
        "    lines = file.readlines() \n",
        "    return lines \n",
        "\n",
        "def preprocess_en(sentence, nlp):\n",
        "    text = sentence.lower()\n",
        "    doc = [token.lemma_ for token in  nlp.tokenizer(text)]\n",
        "    doc = [word for word in doc if word not in stop_words_en]\n",
        "    doc = [word for word in doc if word.isalpha()] \n",
        "    return doc\n",
        "\n",
        "def get_english_sentences(f, nlp, preprocess=True):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences = []\n",
        "\n",
        "  for l in lines:\n",
        "    if preprocess: \n",
        "      sentence = preprocess_en(l, nlp)\n",
        "    else: \n",
        "      sentence = [token.lemma_ for token in  nlp.tokenizer(l)]\n",
        "      sentence = list(filter(None, sentence))\n",
        "    sentences.append(sentence)\n",
        "\n",
        "  return sentences\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1930-s6fN7T",
        "colab_type": "text"
      },
      "source": [
        "### Chinese Models Setup\n",
        "\n",
        "Download Chinese stopwords:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O14b4JeHcNhB",
        "colab_type": "code",
        "outputId": "4f12a66f-b4a6-4438-ba29-cce1515a8ab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "!wget -c https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-28 09:25:56--  https://github.com/Tony607/Chinese_sentiment_analysis/blob/master/data/chinese_stop_words.txt\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘chinese_stop_words.txt’\n",
            "\n",
            "chinese_stop_words.     [   <=>              ] 417.16K   451KB/s    in 0.9s    \n",
            "\n",
            "2020-02-28 09:25:58 (451 KB/s) - ‘chinese_stop_words.txt’ saved [427175]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo5vhydafzC5",
        "colab_type": "text"
      },
      "source": [
        "Functions for processing Chinese dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s27T7LXlf4sG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import jieba\n",
        "import gensim \n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "stop_words = [ line.rstrip() for line in open('./chinese_stop_words.txt',\"r\", encoding=\"utf-8\") ]\n",
        "\n",
        "def processing_zh(sentence):\n",
        "  seg_list = jieba.lcut(sentence,cut_all=True)\n",
        "  doc = [word for word in seg_list if word not in stop_words]\n",
        "  docs = [e for e in doc if e.isalnum()]\n",
        "  return docs\n",
        "\n",
        "\n",
        "def get_chinese_sentences(f, preprocess=True):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  sentences =[]\n",
        "  for l in lines:\n",
        "    if preprocess: \n",
        "      sent  = processing_zh(l)\n",
        "    else: \n",
        "      sent = jieba.lcut(l,cut_all=True)\n",
        "      sent = list(filter(None, sent))\n",
        "    sentences.append(sent)\n",
        "  return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI0BAckemQOU",
        "colab_type": "text"
      },
      "source": [
        "## Feature Extraction: Complexity / Fluency of Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHdd5mVt_57c",
        "colab_type": "text"
      },
      "source": [
        "Extract number of tokens in source segment and target segment\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHNM5UqeDq6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_num_token(data_set_token):\n",
        "  num_tokens = []\n",
        "  for sentence in data_set_token: \n",
        "    num_tokens.append(len(sentence))\n",
        "  return num_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuWG6_yeGgWp",
        "colab_type": "text"
      },
      "source": [
        "N-gram language model probability of source segment using the source side of the parallel corpus used to train the MT system as LM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oil351YZGqIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "def get_lm_prob_src(dataset):\n",
        "\n",
        "  num_src_prob = []\n",
        "  model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "  # Count frequency of co-occurance and store in dictionary \n",
        "  for sentence in dataset:\n",
        "      for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "          model[(w1, w2)][w3] += 1\n",
        "\n",
        "  # Transform the counts to probabilities over total count \n",
        "  for w1_w2 in model:\n",
        "      total_count = float(sum(model[w1_w2].values()))\n",
        "      for w3 in model[w1_w2]:\n",
        "          model[w1_w2][w3] /= total_count\n",
        "\n",
        "  # Calculate the trigram estimate of sentence probabilities  \n",
        "  # p(Sentence) = P(W1, ... Wn)\n",
        "  for sentence in dataset:\n",
        "    sentence_prob = 1\n",
        "    for i in range(len(sentence) - 2):\n",
        "      sentence_prob = sentence_prob * dict(model[sentence[i], sentence[i + 1]])[sentence[i + 2]]\n",
        "    num_src_prob.append(sentence_prob)\n",
        "\n",
        "  return num_src_prob\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUrovyNYX7NT",
        "colab_type": "text"
      },
      "source": [
        "Load the simplified Chinese corpus from Leipzig Uni to build the LM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOi_zpyWX7V4",
        "colab_type": "code",
        "outputId": "e4d4617a-ca4d-4626-87bc-ebf50f403819",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "# Load the Chinese Corpus\n",
        "\n",
        "!wget http://pcai056.informatik.uni-leipzig.de/downloads/corpora/zho-simp-tw_web_2014_10K.tar.gz\n",
        "!tar -zxvf zho-simp-tw_web_2014_10K.tar.gz\n",
        "\n",
        "zho_sentence_path = \"./zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-sentences.txt\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-28 09:26:00--  http://pcai056.informatik.uni-leipzig.de/downloads/corpora/zho-simp-tw_web_2014_10K.tar.gz\n",
            "Resolving pcai056.informatik.uni-leipzig.de (pcai056.informatik.uni-leipzig.de)... 139.18.2.216\n",
            "Connecting to pcai056.informatik.uni-leipzig.de (pcai056.informatik.uni-leipzig.de)|139.18.2.216|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3915364 (3.7M) [application/x-gzip]\n",
            "Saving to: ‘zho-simp-tw_web_2014_10K.tar.gz’\n",
            "\n",
            "zho-simp-tw_web_201 100%[===================>]   3.73M  1.72MB/s    in 2.2s    \n",
            "\n",
            "2020-02-28 09:26:03 (1.72 MB/s) - ‘zho-simp-tw_web_2014_10K.tar.gz’ saved [3915364/3915364]\n",
            "\n",
            "zho-simp-tw_web_2014_10K/\n",
            "zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-sources.txt\n",
            "zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-words.txt\n",
            "zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-inv_w.txt\n",
            "zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-sentences.txt\n",
            "zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-import.sql\n",
            "zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-co_n.txt\n",
            "zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-inv_so.txt\n",
            "zho-simp-tw_web_2014_10K/zho-simp-tw_web_2014_10K-co_s.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ3WXiS4gtoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_process_zho_chinese_sentences(f):\n",
        "  file = open(f) \n",
        "  lines = file.readlines() \n",
        "  lines = lines[1:]\n",
        "  sentences = []\n",
        "  chop_counter = 1\n",
        "  for i, l in enumerate(lines):\n",
        "    if i >= pow(10, chop_counter): \n",
        "      chop_counter += 1\n",
        "    processed_sentence = processing_zh(l[(chop_counter + 1):])\n",
        "    sentences.append(processed_sentence)\n",
        "  return sentences\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ4-Bq-PoKOa",
        "colab_type": "text"
      },
      "source": [
        "Build N Gram Language Model probability of target segment with larger dataset and input mt corpus "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j-_3c1SoIh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "def get_lm_prob_target(dataset):\n",
        "\n",
        "  processed_sentences = pre_process_zho_chinese_sentences(zho_sentence_path)\n",
        "\n",
        "  processed_sentences.extend(dataset)\n",
        "\n",
        "  num_mt_prob = []\n",
        "  model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "  # Count frequency of co-occurance and store in dictionary\n",
        "  for sentence in processed_sentences:\n",
        "      for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "          model[(w1, w2)][w3] += 1\n",
        "\n",
        "  # Transform the counts to probabilities over total count \n",
        "  for w1_w2 in model:\n",
        "      total_count = float(sum(model[w1_w2].values()))\n",
        "      for w3 in model[w1_w2]:\n",
        "          model[w1_w2][w3] /= total_count\n",
        "\n",
        "  # Calculate the trigram estimate of sentence probabilities  \n",
        "  # p(Sentence) = P(W1, ... Wn)\n",
        "  for sentence in dataset:\n",
        "    for i in range(len(sentence) - 2):\n",
        "      sentence_prob = 1\n",
        "      sentence_prob = dict(model[sentence[i], sentence[i + 1]])[sentence[i + 2]]\n",
        "    num_mt_prob.append(sentence_prob)\n",
        "\n",
        "  return num_mt_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTCEB_wgql5e",
        "colab_type": "text"
      },
      "source": [
        "# Feature Extraction: Adequacy of Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uFoIGlWrQ1X",
        "colab_type": "text"
      },
      "source": [
        "Ratio of number of tokens in source and target segments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa3hfkzlrWLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def token_ratio_src_mt(src, mt):\n",
        "  if mt == 0 or src == 0: \n",
        "    return 0\n",
        "  return src / mt\n",
        "\n",
        "def get_ratio_num_token(num_src_tokens, num_mt_tokens):\n",
        "  ratio_token_src_mt = list(map(token_ratio_src_mt, num_src_tokens, num_mt_tokens)) \n",
        "  return ratio_token_src_mt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3ZxBUYzvzbr",
        "colab_type": "text"
      },
      "source": [
        "Ratio of brackets and punctuation symbols in source and target segments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guhmtlib7dM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections as ct\n",
        "\n",
        "def get_punc_ratio_sent(src_sentence, mt_sentence):\n",
        "\n",
        "  src_dict = {c:val for c, val in ct.Counter(src_sentence).items() if c in string.punctuation}\n",
        "  mt_dict = {c:val for c, val in ct.Counter(mt_sentence).items() if c in string.punctuation}\n",
        "\n",
        "  if sum(mt_dict.values())!= 0: \n",
        "    return sum(src_dict.values()) / sum(mt_dict.values()) \n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def get_ratio_punc(pro_dataset_src, pro_dataset_mt):\n",
        "  ratio_punc_src_mt = list(map(get_punc_ratio_sent, pro_dataset_src, pro_dataset_mt))\n",
        "  return ratio_punc_src_mt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxdaSbeR-6Pr",
        "colab_type": "text"
      },
      "source": [
        "Ratio of open class words, closed class words and other in the source & target segments defined in https://universaldependencies.org/u/pos/\n",
        "\n",
        "Ratio of percentage of nouns / verbs etc... in the source and target segments\n",
        "\n",
        "Difference between the depth of the syntactic trees of the source and target segments "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pfe-9ed-6YP",
        "colab_type": "code",
        "outputId": "698400ad-bc8d-41d4-c109-fceb6db01bb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 909
        }
      },
      "source": [
        "import stanfordnlp\n",
        "import warnings\n",
        "from itertools import chain \n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "nlp_stan_en = stanfordnlp.Pipeline(lang=\"en\", processors=\"tokenize,mwt,pos,lemma,depparse\", use_gpu=True)\n",
        "nlp_stan_chinese = stanfordnlp.Pipeline(lang=\"zh\", processors=\"tokenize,mwt,pos,lemma,depparse\", use_gpu=True)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "open_class_words = [\"ADJ\", \"ADV\", \"INTJ\", \"NOUN\", \"PROPN\", \"VERB\"]\n",
        "closed_class_words = [\"ADP\", \"AUX\", \"CCONJ\", \"DET\", \"NUM\", \"PART\", \"PRON\", \"SCONJ\"]\n",
        "other_words = [\"PUNCT\", \"SYM\", \"X\"]\n",
        "\n",
        "all_tag_list = list(chain(open_class_words, closed_class_words, other_words))\n",
        "\n",
        "def get_coarse_tag_count(sentence): \n",
        "\n",
        "  sent_open_class_words = 0\n",
        "  sent_closed_class_words = 0\n",
        "  sent_other_words = 0\n",
        "\n",
        "  for word in sentence:\n",
        "    if word.upos in open_class_words:\n",
        "      sent_open_class_words += 1\n",
        "    elif word.upos in closed_class_words:\n",
        "      sent_closed_class_words += 1\n",
        "    else: \n",
        "      sent_other_words += 1\n",
        "\n",
        "  return sent_open_class_words, sent_closed_class_words, sent_other_words\n",
        "\n",
        "def get_fine_tag_count(sentence): \n",
        "\n",
        "  tag_dict = {\n",
        "    \"ADJ\": 0, \n",
        "    \"ADV\": 0,\n",
        "    \"INTJ\": 0, \n",
        "    \"NOUN\": 0, \n",
        "    \"PROPN\": 0, \n",
        "    \"VERB\": 0,\n",
        "    \"ADP\": 0, \n",
        "    \"AUX\": 0,\n",
        "    \"CCONJ\": 0, \n",
        "    \"DET\": 0, \n",
        "    \"NUM\": 0, \n",
        "    \"PART\": 0, \n",
        "    \"PRON\": 0, \n",
        "    \"SCONJ\": 0,\n",
        "    \"PUNCT\": 0, \n",
        "    \"SYM\": 0, \n",
        "    \"X\": 0\n",
        "  }\n",
        "\n",
        "  tag_list = [word.upos for word in sentence] \n",
        "\n",
        "  sentence_dict = dict(Counter(tag_list))\n",
        "  \n",
        "  tag_dict.update(sentence_dict) \n",
        "  return tag_dict\n",
        "\n",
        "def gen_tree(nodes, root):\n",
        "  tree = defaultdict(dict)\n",
        "\n",
        "  for child, parent in nodes: \n",
        "    tree[parent][child] = tree[child]\n",
        "\n",
        "  try:\n",
        "    children, parents = zip(*nodes)\n",
        "    root = set(parents).difference(children).pop()\n",
        "  except:\n",
        "    print(nodes)\n",
        "    raise NameError('HiThere')\n",
        "\n",
        "  return {root: tree[root]}\n",
        "\n",
        "def dict_depth(tree_dict): \n",
        "    if isinstance(tree_dict, dict):    \n",
        "        return 1 + (max(map(dict_depth, tree_dict.values())) \n",
        "                                    if tree_dict else 0) \n",
        "    return 0\n",
        "\n",
        "def get_dependency_count(sentence): \n",
        "\n",
        "  root_relation = \"root\"\n",
        "  nodes = []\n",
        "  for word in sentence: \n",
        "    if word.dependency_relation == 'root':\n",
        "      root_relation = word.text\n",
        "    else: \n",
        "      nodes.append((word.text, word.governor))\n",
        "  \n",
        "  if not nodes: \n",
        "    return {}\n",
        "\n",
        "  tree_dict = gen_tree(nodes, root_relation)\n",
        "  return tree_dict\n",
        "\n",
        "\n",
        "def get_tag_dep_ratio(src_sentence, mt_sentence):\n",
        "\n",
        "  # Process sentences\n",
        "  src_doc = nlp_stan_en(src_sentence)\n",
        "  mt_doc = nlp_stan_chinese(mt_sentence)\n",
        "\n",
        "  # Get dependency counts \n",
        "  src_tree_dict = get_dependency_count(src_doc.sentences[0].words)\n",
        "  mt_tree_dict = get_dependency_count(mt_doc.sentences[0].words) \n",
        "\n",
        "  depth_diff = abs(dict_depth(src_tree_dict) - dict_depth(mt_tree_dict))\n",
        "\n",
        "  # Get course tag ratios\n",
        "  src_oc_w, src_cc_w, src_o_w = get_coarse_tag_count(src_doc.sentences[0].words)\n",
        "  mt_oc_w, mt_cc_w, mt_o_w = get_coarse_tag_count(mt_doc.sentences[0].words)\n",
        "\n",
        "  oc_ratio = src_oc_w / mt_oc_w if mt_oc_w != 0 else 0 \n",
        "  cc_ratio = src_cc_w / mt_cc_w if mt_cc_w != 0 else 0 \n",
        "  o_ratio = src_o_w / mt_o_w if mt_o_w != 0 else 0 \n",
        "\n",
        "  # Get fine tag ratios\n",
        "  src_tag_dict = get_fine_tag_count(src_doc.sentences[0].words)\n",
        "  mt_tag_dict = get_fine_tag_count(mt_doc.sentences[0].words)\n",
        "\n",
        "  ratio_tags_fine_grain_sent = []\n",
        "  for tag in all_tag_list:\n",
        "    ratio = src_tag_dict[tag] / mt_tag_dict[tag] if mt_tag_dict[tag] != 0 else 0 \n",
        "    ratio_tags_fine_grain_sent.append(ratio)\n",
        "\n",
        "  ratio_dict = {\n",
        "      \"oc_ratio\": oc_ratio, \n",
        "      \"cc_ratio\": cc_ratio,\n",
        "      \"o_ratio\": o_ratio,\n",
        "      \"ratio_tags_fine_grain_sent\": ratio_tags_fine_grain_sent,\n",
        "      \"depth_diff\": depth_diff\n",
        "  }\n",
        "\n",
        "  return ratio_dict\n",
        "\n",
        "def get_ratio_tags_depend(raw_dataset_src, raw_dataset_mt):\n",
        "\n",
        "  ratio_list_of_dict = list(map(get_tag_dep_ratio, raw_dataset_src, raw_dataset_mt))\n",
        "\n",
        "  ratio_oc_tags = [] \n",
        "  ratio_cc_tags = []\n",
        "  ratio_o_tags = []\n",
        "  ratio_tags_fine = []\n",
        "  depth_diff = []\n",
        "\n",
        "  for ratio_dict in ratio_list_of_dict:\n",
        "    ratio_oc_tags.append(ratio_dict['oc_ratio'])\n",
        "    ratio_cc_tags.append(ratio_dict['cc_ratio'])\n",
        "    ratio_o_tags.append(ratio_dict['o_ratio'])\n",
        "    ratio_tags_fine.append(ratio_dict['ratio_tags_fine_grain_sent'])\n",
        "    depth_diff.append(ratio_dict['depth_diff'])\n",
        "\n",
        "  return ratio_oc_tags, ratio_cc_tags, ratio_o_tags, ratio_tags_fine, depth_diff\n",
        "\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: lemma\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "[Running seq2seq lemmatizer with edit classifier]\n",
            "---\n",
            "Loading: depparse\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_parser.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n",
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/zh_gsd_models/zh_gsd_tokenizer.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/zh_gsd_models/zh_gsd_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
            "---\n",
            "Loading: lemma\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/zh_gsd_models/zh_gsd_lemmatizer.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
            "Building an attentional Seq2Seq model...\n",
            "Using a Bi-LSTM encoder\n",
            "Using soft attention for LSTM.\n",
            "Finetune all embeddings.\n",
            "[Running seq2seq lemmatizer with edit classifier]\n",
            "---\n",
            "Loading: depparse\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/zh_gsd_models/zh_gsd_parser.pt', 'pretrain_path': '/root/stanfordnlp_resources/zh_gsd_models/zh_gsd.pretrain.pt', 'lang': 'zh', 'shorthand': 'zh_gsd', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl8o0sxUXku_",
        "colab_type": "text"
      },
      "source": [
        "# Generate the feature vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXTJWa5YpJj7",
        "colab_type": "text"
      },
      "source": [
        "Load the data sets to generate the feature vectors "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ6nLoxEmWN-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "48595db8-1b60-4aa6-83cc-9862939fecd2"
      },
      "source": [
        "import spacy\n",
        "import torchtext\n",
        "import numpy as np\n",
        "from torchtext import data\n",
        "\n",
        "# Train \n",
        "zh_train_mt = get_chinese_sentences(\"./train.enzh.mt\", True)\n",
        "zh_train_src = get_english_sentences(\"./train.enzh.src\", nlp_en, True)\n",
        "\n",
        "f_train_scores = open(\"./train.enzh.scores\", 'r')\n",
        "zh_train_scores = f_train_scores.readlines()\n",
        "\n",
        "pro_zh_train_mt = get_chinese_sentences(\"./train.enzh.mt\", False)\n",
        "pro_zh_train_src = get_english_sentences(\"./train.enzh.src\", nlp_en, False)\n",
        "\n",
        "raw_zh_train_mt = get_sentence_array(\"./train.enzh.mt\")\n",
        "raw_zh_train_src = get_sentence_array(\"./train.enzh.src\")\n",
        "\n",
        "# Validation\n",
        "zh_val_mt = get_chinese_sentences(\"./dev.enzh.mt\")\n",
        "zh_val_src = get_english_sentences(\"./dev.enzh.src\", nlp_en)\n",
        "\n",
        "f_val_scores = open(\"./dev.enzh.scores\", 'r')\n",
        "zh_val_scores = f_val_scores.readlines()\n",
        "\n",
        "pro_zh_val_mt = get_chinese_sentences(\"./dev.enzh.mt\", False)\n",
        "pro_zh_val_src = get_english_sentences(\"./dev.enzh.src\", nlp_en, False)\n",
        "\n",
        "raw_zh_val_mt = get_sentence_array(\"./dev.enzh.mt\")\n",
        "raw_zh_val_src = get_sentence_array(\"./dev.enzh.src\")\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.008 seconds.\n",
            "Prefix dict has been built successfully.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuc9yuqnYEMO",
        "colab_type": "text"
      },
      "source": [
        "Generate the complexity feature set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqnLjljqYDcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_complexity_features(dataset):\n",
        "\n",
        "  num_token_src = get_num_token(dataset)\n",
        "\n",
        "  # Normalise\n",
        "  num_token_src = (num_token_src - np.min(num_token_src)) / (np.max(num_token_src) - np.min(num_token_src))\n",
        "\n",
        "  lm_prob_src = get_lm_prob_src(dataset)\n",
        "  return num_token_src, lm_prob_src\n",
        "\n",
        "num_token_src_train, lm_prob_src_train = gen_complexity_features(zh_train_src)\n",
        "num_token_src_val, lm_prob_src_val = gen_complexity_features(zh_val_src)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G24SmhLb42QL",
        "colab_type": "text"
      },
      "source": [
        "Generate the fluency feature set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvcCMbnK5J7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_fluency_features(dataset):\n",
        "\n",
        "  num_token_mt = get_num_token(dataset)\n",
        "  \n",
        "  # Normalise \n",
        "  num_token_mt = (num_token_mt - np.min(num_token_mt)) / (np.max(num_token_mt) - np.min(num_token_mt))\n",
        "\n",
        "  lm_prob_mt = get_lm_prob_target(dataset)\n",
        "  return num_token_mt, lm_prob_mt\n",
        "\n",
        "num_token_mt_train, lm_prob_mt_train = gen_fluency_features(zh_train_mt)\n",
        "num_token_mt_val, lm_prob_mt_val = gen_fluency_features(zh_val_mt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLgENJczZrjW",
        "colab_type": "text"
      },
      "source": [
        "Generate the adaquecy feature set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iMjxIDbit-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen_adaquecy_simple_features(pro_dataset_src, pro_dataset_mt, num_token_src, num_token_mt):\n",
        "\n",
        "  ratio_token_src_mt = get_ratio_num_token(num_token_src, num_token_mt)\n",
        "  ratio_punc_src_mt = get_ratio_punc(pro_dataset_src, pro_dataset_mt)\n",
        "\n",
        "  return ratio_token_src_mt, ratio_punc_src_mt\n",
        "\n",
        "ratio_token_src_mt_train, ratio_punc_src_mt_train = \\\n",
        "  gen_adaquecy_simple_features(pro_zh_train_src, pro_zh_train_mt, \\\n",
        "                                num_token_src_train, num_token_mt_train)\n",
        "ratio_token_src_mt_val, ratio_punc_src_mt_val = \\\n",
        "  gen_adaquecy_simple_features(pro_zh_train_src, pro_zh_train_mt, \\\n",
        "                                num_token_src_val, num_token_mt_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNpRYNRBDG93",
        "colab_type": "text"
      },
      "source": [
        "Note: The cell below may take 20 mins to run "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uFyAsrsZrvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train \n",
        "ratio_oc_tags_train, ratio_cc_tags_train, ratio_o_tags_train, \\\n",
        " ratio_tags_fine_train, depth_diff_train =  \\\n",
        " get_ratio_tags_depend(raw_zh_train_src, raw_zh_train_mt)\n",
        "\n",
        "# Validation \n",
        "ratio_oc_tags_val, ratio_cc_tags_val, ratio_o_tags_val, \\\n",
        " ratio_tags_fine_val, depth_diff_val =  \\\n",
        " get_ratio_tags_depend(raw_zh_val_src, raw_zh_val_mt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va1V_LnY4s27",
        "colab_type": "text"
      },
      "source": [
        "Combine the features to create an single feature vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIGo4BqU4tIW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_feature_vectors(num_token_src, lm_prob_src, \n",
        "                        num_token_mt, lm_prob_mt, \n",
        "                        ratio_token_src_mt, ratio_punc_src_mt, \n",
        "                        ratio_oc_tags, ratio_cc_tags, ratio_o_tags, \n",
        "                        ratio_tags_fine, depth_diff):\n",
        "  feature_vector_list = []\n",
        "  for i in range(0, len(num_token_src)):\n",
        "    feature_vector = [\n",
        "      num_token_src[i],\n",
        "      lm_prob_src[i],\n",
        "      num_token_mt[i],\n",
        "      lm_prob_mt[i],\n",
        "      ratio_token_src_mt[i],\n",
        "      ratio_punc_src_mt[i],\n",
        "      ratio_oc_tags[i],\n",
        "      ratio_cc_tags[i],\n",
        "      ratio_o_tags[i],    \n",
        "      depth_diff[i]          \n",
        "    ]\n",
        "    feature_vector.extend(ratio_tags_fine[i])\n",
        "    feature_vector_list.append(feature_vector)\n",
        "  return feature_vector_list\n",
        "  \n",
        "feature_vectors_train = get_feature_vectors(num_token_src_train, lm_prob_src_train, \n",
        "                        num_token_mt_train, lm_prob_mt_train, \n",
        "                        ratio_token_src_mt_train, ratio_punc_src_mt_train,  \n",
        "                        ratio_oc_tags_train, ratio_cc_tags_train, ratio_o_tags_train, \n",
        "                        ratio_tags_fine_train, depth_diff_train)\n",
        "                    \n",
        "feature_vectors_val = get_feature_vectors(num_token_src_val, lm_prob_src_val, \n",
        "                        num_token_mt_val, lm_prob_mt_val, \n",
        "                        ratio_token_src_mt_val, ratio_punc_src_mt_val,  \n",
        "                        ratio_oc_tags_val, ratio_cc_tags_val, ratio_o_tags_val, \n",
        "                        ratio_tags_fine_val, depth_diff_val)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9hLFzLJHDrZ",
        "colab_type": "text"
      },
      "source": [
        "Create the training and label sets for train and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUGFP6_6HKsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = np.array(feature_vectors_train)\n",
        "y_train = np.array(zh_train_scores).astype(float)\n",
        "\n",
        "X_val = np.array(feature_vectors_val)\n",
        "y_val = np.array(zh_val_scores).astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsuPvgMCjT_u",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Regression Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzCLyRD6HyBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tplA9CizYq1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "train_sc_X = StandardScaler()\n",
        "train_sc_y = StandardScaler()\n",
        "val_sc_X = StandardScaler()\n",
        "val_sc_y = StandardScaler()\n",
        "\n",
        "X_train = train_sc_X.fit_transform(X_train)\n",
        "y_train = train_sc_y.fit_transform(y_train.reshape(-1, 1))\n",
        "\n",
        "X_val = val_sc_X.fit_transform(X_val)\n",
        "y_val = val_sc_y.fit_transform(y_val.reshape(-1, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SELo0xgH0OO",
        "colab_type": "code",
        "outputId": "d762f66f-cb20-4ef7-cbaa-08b783f15a54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "from scipy.stats.stats import pearsonr\n",
        "\n",
        "for k in ['linear','poly','rbf','sigmoid']:\n",
        "    clf_t = SVR(kernel=k)\n",
        "    clf_t.fit(X_train, y_train)\n",
        "    print(k)\n",
        "    predictions = clf_t.predict(X_val)\n",
        "    pearson = pearsonr(y_val_zh, predictions)\n",
        "    print(f'RMSE: {rmse(predictions,y_val)} Pearson {pearson[0]}')\n",
        "    print()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "linear\n",
            "RMSE: 1.0598827645427986 Pearson 0.15027546828328553\n",
            "\n",
            "poly\n",
            "RMSE: 1.1887869940545444 Pearson 0.09471634868357552\n",
            "\n",
            "rbf\n",
            "RMSE: 1.0723557737713991 Pearson 0.11011747669688919\n",
            "\n",
            "sigmoid\n",
            "RMSE: 18.44201824227454 Pearson -0.08312457718447623\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLV1j47Vo97i",
        "colab_type": "text"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XimIq82so7Op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def writeScores(scores):\n",
        "    fn = \"predictions.txt\"\n",
        "    print(\"\")\n",
        "    with open(fn, 'w') as output_file:\n",
        "        for idx,x in enumerate(scores):\n",
        "            #out =  metrics[idx]+\":\"+str(\"{0:.2f}\".format(x))+\"\\n\"\n",
        "            #print(out)\n",
        "            output_file.write(f\"{x}\\n\")\n",
        "\n",
        "\n",
        "def downloadScores(method_name, scores):\n",
        "  writeScores(scores)\n",
        "  with ZipFile(f\"en-zh_{method_name}.zip\", \"w\") as newzip:\n",
        "    newzip.write(\"predictions.txt\")\n",
        "  \n",
        "  files.download(f\"en-zh_{method_name}.zip\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aXbG85RfRrk",
        "colab_type": "text"
      },
      "source": [
        "# Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h25bXp5ikoMI",
        "colab_type": "text"
      },
      "source": [
        "Load Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUIoHn7efR-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test \n",
        "zh_test_mt = get_chinese_sentences(\"./test.enzh.mt\", True)\n",
        "zh_test_src = get_english_sentences(\"./test.enzh.src\", nlp_en, True)\n",
        "\n",
        "pro_zh_test_mt = get_chinese_sentences(\"./test.enzh.mt\", False)\n",
        "pro_zh_test_src = get_english_sentences(\"./test.enzh.src\", nlp_en, False)\n",
        "\n",
        "raw_zh_test_mt = get_sentence_array(\"./test.enzh.mt\")\n",
        "raw_zh_test_src = get_sentence_array(\"./test.enzh.src\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6NAehiAkpo8",
        "colab_type": "text"
      },
      "source": [
        "Generate Feature Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwawMx0YktqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Complexity \n",
        "num_token_src_test, lm_prob_src_test = gen_complexity_features(zh_test_src)\n",
        "# Fluency \n",
        "num_token_mt_test, lm_prob_mt_test = gen_fluency_features(zh_test_mt)\n",
        "# Adaquecy \n",
        "ratio_token_src_mt_test, ratio_punc_src_mt_test = \\\n",
        "  gen_adaquecy_simple_features(pro_zh_test_src, pro_zh_test_mt, \\\n",
        "                                num_token_src_test, num_token_mt_test)\n",
        "# This feature may take 10 mins to generate\n",
        "ratio_oc_tags_test, ratio_cc_tags_test, ratio_o_tags_test, \\\n",
        " ratio_tags_fine_test, depth_diff_test=  \\\n",
        " get_ratio_tags_depend(raw_zh_test_src, raw_zh_test_mt)\n",
        "\n",
        " # Generate feature vector   \n",
        "feature_vectors_test = get_feature_vectors(num_token_src_test, lm_prob_src_test, \n",
        "                        num_token_mt_test, lm_prob_mt_test, \n",
        "                        ratio_token_src_mt_test, ratio_punc_src_mt_test,  \n",
        "                        ratio_oc_tags_test, ratio_cc_tags_test, ratio_o_tags_test, \n",
        "                        ratio_tags_fine_test, depth_diff_test)\n",
        "# Test feature vector\n",
        "X_test = np.array(feature_vectors_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0jdmfaacL-d",
        "colab_type": "text"
      },
      "source": [
        "Normalise the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsO6Z5oKcMGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "test_sc_X = StandardScaler()\n",
        "\n",
        "X_test = train_sc_X.fit_transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSJHd7cpllnz",
        "colab_type": "text"
      },
      "source": [
        "Predict using the best model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKbk4gQVllwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clf_zh = SVR(kernel='linear')\n",
        "clf_zh.fit(X_train, y_train)\n",
        "\n",
        "predictions_zh = clf_zh.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNjnY71hm3_W",
        "colab_type": "text"
      },
      "source": [
        "Save the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SUcU_o_m3wX",
        "colab_type": "code",
        "outputId": "2600f5ca-79f1-40f6-93df-e0e505e6cc91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import files\n",
        "from zipfile import ZipFile\n",
        "\n",
        "writeScores(predictions_zh)\n",
        "\n",
        "with ZipFile(\"en-zh_svr.zip\",\"w\") as newzip:\n",
        "\tnewzip.write(\"predictions.txt\")\n",
        " \n",
        "files.download('en-zh_svr.zip')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}